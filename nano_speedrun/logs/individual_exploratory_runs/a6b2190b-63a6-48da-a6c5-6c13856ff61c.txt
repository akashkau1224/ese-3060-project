====================================================================================================
# Training a model for 500 iterations and seeing how the function, validation loss, and training time change over time (every 50 epochs)
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
import math

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, int(config.expansion_factor * config.n_embd), bias=False)
        self.c_proj  = nn.Linear(int(config.expansion_factor * config.n_embd), config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6
    n_embd : int = 768
    expansion_factor : int = 4

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total
        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = '../fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = '../fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # model hyperparams
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6
    n_embd : int = 768

    expansion_factor : int = 4


def run_training_test(args: Hyperparameters, ddp_rank: int, ddp_local_rank: int, ddp_world_size: int,
                      master_process: bool, run_id: str = None, should_log: bool = True, intermediate_log: str = None):
    """
    Run a complete training and testing cycle.

    Args:
        args: Hyperparameters object containing all training configuration
        ddp_rank: Distributed data parallel rank
        ddp_local_rank: Local rank for device assignment
        ddp_world_size: Total number of processes
        master_process: Whether this is the master process (for logging)
        run_id: Optional run ID for logging. If None, generates a new UUID.

    Returns:
        dict: Results containing final validation loss, training time, peak memory, etc.
    """
    # convenience variables
    B, T = args.device_batch_size, args.sequence_length
    # calculate the number of steps to take in the val loop.
    assert args.val_tokens % (B * T * ddp_world_size) == 0
    val_steps = args.val_tokens // (B * T * ddp_world_size)
    # calculate the steps of gradient accumulation required to attain the desired global batch size.
    assert args.batch_size % (B * ddp_world_size) == 0
    train_accumulation_steps = args.batch_size // (B * ddp_world_size)

    # load tokens
    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
    val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
    if master_process:
        print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
        print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
    x, y = train_loader.next_batch()

    # Initialize model
    model = GPT(GPTConfig(vocab_size=args.vocab_size, n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd, expansion_factor=args.expansion_factor))
    model = model.cuda()
    if hasattr(config, "coordinate_descent_tuning"):
        config.coordinate_descent_tuning = True # suggested by @Chillee
    model = torch.compile(model)
    # here we wrap model into DDP container
    model = DDP(model, device_ids=[ddp_local_rank])
    raw_model = model.module # always contains the "raw" unwrapped model
    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

    # init the optimizer(s)
    optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                                   weight_decay=args.weight_decay, fused=True)
    optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
    optimizers = [optimizer1, optimizer2]
    # learning rate decay scheduler (linear warmup and warmdown)
    def get_lr(it):
        assert it <= args.num_iterations
        # 1) linear warmup for warmup_iters steps
        if it < args.warmup_iters:
            return (it+1) / args.warmup_iters
        # 2) constant lr for a while
        elif it < args.num_iterations - args.warmdown_iters:
            return 1.0
        # 3) linear warmdown
        else:
            decay_ratio = (args.num_iterations - it) / args.warmdown_iters
            return decay_ratio
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

    # begin logging
    logfile = None
    if run_id is None:
        run_id = str(uuid.uuid4()) if master_process else None
    if master_process and should_log:
        logdir = 'logs/%s/' % run_id
        os.makedirs(logdir, exist_ok=True)
        logfile = 'logs/%s.txt' % run_id
        # create the log file
        with open(logfile, "w") as f:
            # begin the log by printing this file (the Python code)
            f.write('='*100 + '\n')
            f.write(code)
            f.write('='*100 + '\n')
            # log information about the hardware/software environment this is running on
            # and print the full `nvidia-smi` to file
            f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
            import subprocess
            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            f.write(f'{result.stdout}\n')
            f.write('='*100 + '\n')

    training_time_ms = 0
    final_val_loss = None
    # start the clock
    torch.cuda.synchronize()
    t0 = time.time()
    # begin training
    train_loader.reset()
    for step in range(args.num_iterations + 1):
        last_step = (step == args.num_iterations)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.time()
        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val
        # once in a while evaluate the validation dataset
        if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.time() - t0)
            # run validation batches
            model.eval()
            val_loader.reset()
            val_loss = 0.0
            for _ in range(val_steps):
                x_val, y_val = val_loader.next_batch()
                with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                    _, loss = model(x_val, y_val, return_logits=False)
                    val_loss += loss.detach()
                    del loss
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            val_loss /= val_steps
            final_val_loss = val_loss.item() if last_step else final_val_loss
            # log val loss to console and to logfile
            if master_process and should_log == True:
                print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
                if last_step:
                    with open(logfile, "a") as f:
                        f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
            if master_process and intermediate_log:
                with open(intermediate_log, "a") as f:
                    f.write(f"Step {step} of {args.num_iterations} completed: training_time_ms={training_time_ms:.0f}ms\n")
                    f.write(f"Val loss: {val_loss:.4f}\n")
                    f.write(f"Function value: {function(val_loss, training_time_ms):.4f}\n")
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.time()

        if master_process and should_log and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.time() - t0)
            # save the state of the training process
            log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.time()

        # bit confusing: we want to make sure to eval on 0th iteration
        # but also after the very last iteration. so we loop for step <= num_iterations
        # instead of just < num_iterations (one extra due to <=), only to do
        # the validation/sampling one last time, and then we break right here as we're done.
        if last_step:
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        model.train()
        for i in range(1, train_accumulation_steps+1):
            # forward pass
            with ctx:
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
            # advance the dataset for the next batch
            x, y = train_loader.next_batch()
            # backward pass
            if i < train_accumulation_steps:
                with model.no_sync(): # there's no need to sync gradients every accumulation step
                    loss.backward()
            else:
                loss.backward() # just sync on the last step
        for p in model.parameters():
            p.grad /= train_accumulation_steps
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # --------------- TRAINING SECTION END -------------------
        # everything that follows now is just diagnostics, prints, logging, etc.

        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
        if master_process and should_log:
            approx_time = training_time_ms + 1000 * (time.time() - t0)
            print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
            with open(logfile, "a") as f:
                f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

    # Collect results
    peak_memory_mib = torch.cuda.max_memory_allocated() // 1024 // 1024
    if master_process and should_log:
        print(f"peak memory consumption: {peak_memory_mib} MiB")

    results = {
        'final_val_loss': final_val_loss,
        'training_time_ms': training_time_ms,
        'peak_memory_mib': peak_memory_mib,
        'run_id': run_id if master_process else None,
        'expansion_factor': args.expansion_factor
    }

    if master_process and should_log:
        with open(logfile, "a") as f:
            f.write(f"\n\n==============================================\nFinal Results for run_id: {run_id}, expansion_factor: {args.expansion_factor}, final_val_loss: {final_val_loss}, training_time_ms: {training_time_ms}, peak_memory_mib: {peak_memory_mib}\n==============================================\n\n")

    return results

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

def function(validation_loss: float, training_time: float) -> float:
    return (0.965 * math.log(validation_loss) + 0.035 * math.log(training_time))
# Single test
# args = Hyperparameters()
# results = run_training_test(args, ddp_rank, ddp_local_rank, ddp_world_size, master_process)
# Not logging individual test results for initial testing

long_test_log = "./logs/function_testing.txt"
if master_process:
    with open(long_test_log, "w") as f:
        f.write(f"More graunuar testing with custom functionlog\n")
        f.write(f"==============================================\n")
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n\n')
        f.write(f"==============================================\n")
        f.write(f"Testing expansion factors\n")
        f.write(f"==============================================\n")
# Extreme examples to test timing of training a model with different expansion factors
for i in [3.1, 3.5, 3.8, 4.0]:
    if master_process:
        with open(long_test_log, "a") as f:
            f.write(f"==============================================\n")
            f.write(f"Testing expansion factor {i} with 500 iterations\n")
            f.write(f"==============================================\n")
    args = Hyperparameters(expansion_factor=i, num_iterations=500, val_loss_every=50)
    results = run_training_test(args, ddp_rank, ddp_local_rank, ddp_world_size, master_process, should_log=True, intermediate_log=long_test_log)
    print(f"Test with expansion factor {i} completed: final_val_loss={results['final_val_loss']:.4f}, training_time={results['training_time_ms']:.0f}ms")

    function_value = function(results['final_val_loss'], results['training_time_ms'])
    print(f"Function value for expansion factor {i}: {function_value:.4f}")
    if master_process:
        with open(long_test_log, "a") as f:
            f.write(f"==============================================\n")
            f.write(f"Test with expansion factor {i} completed: final_val_loss={results['final_val_loss']:.4f}, training_time={results['training_time_ms']:.0f}ms\n")
            f.write(f"Function value for expansion factor {i}: {function_value:.4f}\n")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  2 21:30:38 2025
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40                     On  |   00000000:01:00.0 Off |                    0 |
| N/A   41C    P0             87W /  300W |   42359MiB /  46068MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA L40                     On  |   00000000:A1:00.0 Off |                    0 |
| N/A   39C    P0             91W /  300W |   42355MiB /  46068MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA L40                     On  |   00000000:C1:00.0 Off |                    0 |
| N/A   39C    P0             89W /  300W |   42357MiB /  46068MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA L40                     On  |   00000000:E1:00.0 Off |                    0 |
| N/A   40C    P0             92W /  300W |   42357MiB /  46068MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:1/500 train_loss:16.0232 train_time:76409ms step_avg:nanms
step:2/500 train_loss:9.5644 train_time:78926ms step_avg:nanms
step:3/500 train_loss:8.9425 train_time:81465ms step_avg:nanms
step:4/500 train_loss:8.4825 train_time:83974ms step_avg:nanms
step:5/500 train_loss:8.2196 train_time:86459ms step_avg:nanms
step:6/500 train_loss:7.6793 train_time:88960ms step_avg:nanms
step:7/500 train_loss:7.7913 train_time:91472ms step_avg:nanms
step:8/500 train_loss:7.5189 train_time:93787ms step_avg:nanms
step:9/500 train_loss:7.3148 train_time:96358ms step_avg:nanms
step:10/500 train_loss:7.2160 train_time:98840ms step_avg:nanms
step:11/500 train_loss:7.1416 train_time:2415ms step_avg:nanms
step:12/500 train_loss:6.9594 train_time:4735ms step_avg:nanms
step:13/500 train_loss:6.9457 train_time:7245ms step_avg:2415.03ms
step:14/500 train_loss:6.9016 train_time:9354ms step_avg:2338.47ms
step:15/500 train_loss:6.8138 train_time:11974ms step_avg:2394.73ms
step:16/500 train_loss:6.8193 train_time:14520ms step_avg:2419.98ms
step:17/500 train_loss:6.8459 train_time:17036ms step_avg:2433.65ms
step:18/500 train_loss:6.6597 train_time:19226ms step_avg:2403.20ms
step:19/500 train_loss:6.6626 train_time:21721ms step_avg:2413.43ms
step:20/500 train_loss:6.3155 train_time:24240ms step_avg:2424.00ms
step:21/500 train_loss:6.7139 train_time:26781ms step_avg:2434.66ms
step:22/500 train_loss:6.9466 train_time:28799ms step_avg:2399.90ms
step:23/500 train_loss:6.5783 train_time:31237ms step_avg:2402.88ms
step:24/500 train_loss:6.6853 train_time:33761ms step_avg:2411.50ms
step:25/500 train_loss:6.4071 train_time:36286ms step_avg:2419.05ms
step:26/500 train_loss:6.3228 train_time:38781ms step_avg:2423.80ms
step:27/500 train_loss:6.4969 train_time:41204ms step_avg:2423.78ms
step:28/500 train_loss:6.1412 train_time:43724ms step_avg:2429.14ms
step:29/500 train_loss:6.4281 train_time:46155ms step_avg:2429.20ms
step:30/500 train_loss:6.2742 train_time:48669ms step_avg:2433.46ms
step:31/500 train_loss:6.2393 train_time:51183ms step_avg:2437.27ms
step:32/500 train_loss:6.0420 train_time:53477ms step_avg:2430.79ms
step:33/500 train_loss:6.4117 train_time:55758ms step_avg:2424.25ms
step:34/500 train_loss:6.2983 train_time:58245ms step_avg:2426.88ms
step:35/500 train_loss:6.4651 train_time:60765ms step_avg:2430.60ms
step:36/500 train_loss:6.3888 train_time:63193ms step_avg:2430.51ms
step:37/500 train_loss:6.2712 train_time:65484ms step_avg:2425.34ms
step:38/500 train_loss:6.1592 train_time:67866ms step_avg:2423.77ms
step:39/500 train_loss:6.2266 train_time:70043ms step_avg:2415.28ms
step:40/500 train_loss:6.1189 train_time:72593ms step_avg:2419.78ms
step:41/500 train_loss:6.1602 train_time:75109ms step_avg:2422.88ms
step:42/500 train_loss:6.0238 train_time:77631ms step_avg:2425.98ms
step:43/500 train_loss:6.1091 train_time:80072ms step_avg:2426.42ms
step:44/500 train_loss:6.1007 train_time:82589ms step_avg:2429.08ms
step:45/500 train_loss:6.2877 train_time:85105ms step_avg:2431.56ms
step:46/500 train_loss:6.0888 train_time:87623ms step_avg:2433.96ms
step:47/500 train_loss:5.9268 train_time:89799ms step_avg:2427.00ms
step:48/500 train_loss:6.1752 train_time:92317ms step_avg:2429.40ms
step:49/500 train_loss:6.0394 train_time:94823ms step_avg:2431.35ms
step:50/500 train_loss:6.1973 train_time:97336ms step_avg:2433.40ms
step:51/500 train_loss:6.0622 train_time:99958ms step_avg:2438.01ms
step:52/500 train_loss:5.9024 train_time:102415ms step_avg:2438.45ms
step:53/500 train_loss:6.0387 train_time:104972ms step_avg:2441.20ms
step:54/500 train_loss:5.9587 train_time:107497ms step_avg:2443.12ms
step:55/500 train_loss:6.2627 train_time:109663ms step_avg:2436.95ms
step:56/500 train_loss:5.9470 train_time:111927ms step_avg:2433.20ms
step:57/500 train_loss:5.8319 train_time:114084ms step_avg:2427.31ms
step:58/500 train_loss:6.0004 train_time:116397ms step_avg:2424.93ms
step:59/500 train_loss:5.9222 train_time:118730ms step_avg:2423.06ms
step:60/500 train_loss:6.0174 train_time:121246ms step_avg:2424.92ms
step:61/500 train_loss:5.8156 train_time:123766ms step_avg:2426.79ms
step:62/500 train_loss:5.9095 train_time:126305ms step_avg:2428.93ms
step:63/500 train_loss:5.8804 train_time:128472ms step_avg:2423.99ms
step:64/500 train_loss:5.9893 train_time:130979ms step_avg:2425.54ms
step:65/500 train_loss:5.7085 train_time:133510ms step_avg:2427.46ms
step:66/500 train_loss:5.8786 train_time:135738ms step_avg:2423.90ms
step:67/500 train_loss:5.7440 train_time:138238ms step_avg:2425.22ms
step:68/500 train_loss:5.9878 train_time:140235ms step_avg:2417.84ms
step:69/500 train_loss:5.6624 train_time:142819ms step_avg:2420.66ms
step:70/500 train_loss:5.6814 train_time:145335ms step_avg:2422.24ms
step:71/500 train_loss:5.8871 train_time:147853ms step_avg:2423.81ms
step:72/500 train_loss:5.8235 train_time:150413ms step_avg:2426.02ms
step:73/500 train_loss:5.7196 train_time:152812ms step_avg:2425.59ms
step:74/500 train_loss:5.8343 train_time:155325ms step_avg:2426.96ms
step:75/500 train_loss:5.7929 train_time:157737ms step_avg:2426.72ms
step:76/500 train_loss:5.7623 train_time:160270ms step_avg:2428.34ms
step:77/500 train_loss:5.8383 train_time:162450ms step_avg:2424.63ms
step:78/500 train_loss:5.8537 train_time:164677ms step_avg:2421.73ms
step:79/500 train_loss:5.7253 train_time:166965ms step_avg:2419.78ms
step:80/500 train_loss:5.8156 train_time:169552ms step_avg:2422.17ms
step:81/500 train_loss:5.5760 train_time:172081ms step_avg:2423.68ms
step:82/500 train_loss:5.7463 train_time:174459ms step_avg:2423.05ms
step:83/500 train_loss:5.7148 train_time:176763ms step_avg:2421.41ms
step:84/500 train_loss:5.6832 train_time:179045ms step_avg:2419.53ms
step:85/500 train_loss:5.5487 train_time:181619ms step_avg:2421.59ms
step:86/500 train_loss:5.7573 train_time:184112ms step_avg:2422.52ms
step:87/500 train_loss:5.6578 train_time:186633ms step_avg:2423.81ms
step:88/500 train_loss:5.7179 train_time:189147ms step_avg:2424.96ms
step:89/500 train_loss:5.7005 train_time:191672ms step_avg:2426.23ms
step:90/500 train_loss:5.6140 train_time:194160ms step_avg:2427.00ms
step:91/500 train_loss:5.6140 train_time:196715ms step_avg:2428.58ms
step:92/500 train_loss:5.7032 train_time:199258ms step_avg:2429.97ms
step:93/500 train_loss:5.5624 train_time:201542ms step_avg:2428.21ms
step:94/500 train_loss:5.5484 train_time:203998ms step_avg:2428.55ms
step:95/500 train_loss:5.5677 train_time:206521ms step_avg:2429.66ms
step:96/500 train_loss:5.4833 train_time:209037ms step_avg:2430.67ms
step:97/500 train_loss:5.5564 train_time:211550ms step_avg:2431.61ms
step:98/500 train_loss:5.4764 train_time:214091ms step_avg:2432.85ms
step:99/500 train_loss:5.6059 train_time:216614ms step_avg:2433.87ms
step:100/500 train_loss:5.5650 train_time:219221ms step_avg:2435.79ms
step:101/500 train_loss:5.4808 train_time:221542ms step_avg:2434.53ms
step:102/500 train_loss:5.5752 train_time:224035ms step_avg:2435.16ms
step:103/500 train_loss:5.5365 train_time:226432ms step_avg:2434.75ms
step:104/500 train_loss:5.3554 train_time:228724ms step_avg:2433.24ms
step:105/500 train_loss:5.4659 train_time:231277ms step_avg:2434.49ms
step:106/500 train_loss:5.6857 train_time:233579ms step_avg:2433.11ms
step:107/500 train_loss:5.4604 train_time:236085ms step_avg:2433.87ms
step:108/500 train_loss:5.2177 train_time:238587ms step_avg:2434.56ms
step:109/500 train_loss:5.4270 train_time:241130ms step_avg:2435.66ms
step:110/500 train_loss:5.3951 train_time:243327ms step_avg:2433.27ms
step:111/500 train_loss:5.3644 train_time:245702ms step_avg:2432.69ms
step:112/500 train_loss:5.4709 train_time:248292ms step_avg:2434.23ms
step:113/500 train_loss:5.3960 train_time:250799ms step_avg:2434.94ms
step:114/500 train_loss:5.2539 train_time:253199ms step_avg:2434.60ms
step:115/500 train_loss:5.4185 train_time:255710ms step_avg:2435.34ms
step:116/500 train_loss:5.2786 train_time:258212ms step_avg:2435.97ms
step:117/500 train_loss:5.2692 train_time:260717ms step_avg:2436.61ms
step:118/500 train_loss:5.3899 train_time:263226ms step_avg:2437.28ms
step:119/500 train_loss:5.3807 train_time:265740ms step_avg:2437.98ms
step:120/500 train_loss:5.2925 train_time:268336ms step_avg:2439.42ms
step:121/500 train_loss:5.1982 train_time:270864ms step_avg:2440.21ms
step:122/500 train_loss:5.2945 train_time:273088ms step_avg:2438.28ms
step:123/500 train_loss:5.1567 train_time:275272ms step_avg:2436.04ms
step:124/500 train_loss:5.4623 train_time:277864ms step_avg:2437.40ms
step:125/500 train_loss:5.3147 train_time:280387ms step_avg:2438.15ms
step:126/500 train_loss:5.2919 train_time:282886ms step_avg:2438.67ms
step:127/500 train_loss:5.3574 train_time:285361ms step_avg:2438.98ms
step:128/500 train_loss:5.2124 train_time:287519ms step_avg:2436.60ms
step:129/500 train_loss:5.4740 train_time:289746ms step_avg:2434.84ms
step:130/500 train_loss:5.2696 train_time:291949ms step_avg:2432.91ms
step:131/500 train_loss:5.2733 train_time:294476ms step_avg:2433.68ms
step:132/500 train_loss:5.2149 train_time:296769ms step_avg:2432.53ms
step:133/500 train_loss:5.2528 train_time:298953ms step_avg:2430.51ms
step:134/500 train_loss:5.1827 train_time:301192ms step_avg:2428.97ms
step:135/500 train_loss:5.2451 train_time:303721ms step_avg:2429.77ms
step:136/500 train_loss:5.0517 train_time:306236ms step_avg:2430.45ms
step:137/500 train_loss:5.2123 train_time:308749ms step_avg:2431.09ms
step:138/500 train_loss:5.1671 train_time:311251ms step_avg:2431.65ms
step:139/500 train_loss:5.1806 train_time:313596ms step_avg:2430.98ms
step:140/500 train_loss:5.2314 train_time:316114ms step_avg:2431.65ms
step:141/500 train_loss:5.1307 train_time:318624ms step_avg:2432.24ms
step:142/500 train_loss:5.2043 train_time:320919ms step_avg:2431.20ms
step:143/500 train_loss:5.0249 train_time:323501ms step_avg:2432.33ms
step:144/500 train_loss:5.1797 train_time:326033ms step_avg:2433.08ms
step:145/500 train_loss:5.1140 train_time:328552ms step_avg:2433.72ms
step:146/500 train_loss:5.0305 train_time:331051ms step_avg:2434.20ms
step:147/500 train_loss:5.1447 train_time:333643ms step_avg:2435.35ms
step:148/500 train_loss:5.1231 train_time:336214ms step_avg:2436.33ms
step:149/500 train_loss:5.1928 train_time:338736ms step_avg:2436.95ms
step:150/500 train_loss:5.1921 train_time:341352ms step_avg:2438.23ms
step:151/500 train_loss:5.1166 train_time:343720ms step_avg:2437.73ms
step:152/500 train_loss:5.0875 train_time:346284ms step_avg:2438.62ms
step:153/500 train_loss:5.1652 train_time:348495ms step_avg:2437.03ms
step:154/500 train_loss:5.1041 train_time:351002ms step_avg:2437.51ms
step:155/500 train_loss:5.0820 train_time:353501ms step_avg:2437.94ms
step:156/500 train_loss:5.0969 train_time:356044ms step_avg:2438.66ms
step:157/500 train_loss:5.2246 train_time:358481ms step_avg:2438.65ms
step:158/500 train_loss:5.0119 train_time:361014ms step_avg:2439.28ms
step:159/500 train_loss:5.0637 train_time:363540ms step_avg:2439.87ms
step:160/500 train_loss:4.9302 train_time:365658ms step_avg:2437.72ms
step:161/500 train_loss:5.0729 train_time:368167ms step_avg:2438.19ms
step:162/500 train_loss:5.1135 train_time:370698ms step_avg:2438.80ms
step:163/500 train_loss:5.1018 train_time:373221ms step_avg:2439.35ms
step:164/500 train_loss:4.9304 train_time:375676ms step_avg:2439.45ms
step:165/500 train_loss:5.0441 train_time:378237ms step_avg:2440.24ms
step:166/500 train_loss:5.1964 train_time:380388ms step_avg:2438.39ms
step:167/500 train_loss:4.9751 train_time:382842ms step_avg:2438.48ms
step:168/500 train_loss:5.0580 train_time:385460ms step_avg:2439.62ms
step:169/500 train_loss:4.9260 train_time:387982ms step_avg:2440.14ms
step:170/500 train_loss:4.8699 train_time:389984ms step_avg:2437.40ms
step:171/500 train_loss:4.9735 train_time:392511ms step_avg:2437.95ms
step:172/500 train_loss:4.9433 train_time:395032ms step_avg:2438.47ms
step:173/500 train_loss:5.0044 train_time:397421ms step_avg:2438.17ms
step:174/500 train_loss:5.1410 train_time:399452ms step_avg:2435.68ms
step:175/500 train_loss:5.0275 train_time:401947ms step_avg:2436.04ms
step:176/500 train_loss:4.8657 train_time:404539ms step_avg:2436.98ms
step:177/500 train_loss:4.8456 train_time:406940ms step_avg:2436.76ms
step:178/500 train_loss:4.8810 train_time:409438ms step_avg:2437.13ms
step:179/500 train_loss:4.9251 train_time:411958ms step_avg:2437.62ms
step:180/500 train_loss:4.9118 train_time:414541ms step_avg:2438.48ms
step:181/500 train_loss:5.0285 train_time:416958ms step_avg:2438.35ms
step:182/500 train_loss:4.9154 train_time:419489ms step_avg:2438.89ms
step:183/500 train_loss:4.8449 train_time:422008ms step_avg:2439.35ms
step:184/500 train_loss:4.8755 train_time:424542ms step_avg:2439.90ms
step:185/500 train_loss:4.9957 train_time:426754ms step_avg:2438.59ms
step:186/500 train_loss:4.8826 train_time:429062ms step_avg:2437.85ms
step:187/500 train_loss:5.1294 train_time:431575ms step_avg:2438.28ms
step:188/500 train_loss:4.9187 train_time:434102ms step_avg:2438.77ms
step:189/500 train_loss:4.8338 train_time:436623ms step_avg:2439.24ms
step:190/500 train_loss:4.9899 train_time:440392ms step_avg:2446.62ms
step:191/500 train_loss:5.0237 train_time:442747ms step_avg:2446.12ms
step:192/500 train_loss:4.8537 train_time:445283ms step_avg:2446.61ms
step:193/500 train_loss:4.8859 train_time:447793ms step_avg:2446.96ms
step:194/500 train_loss:4.8723 train_time:449970ms step_avg:2445.49ms
step:195/500 train_loss:4.8487 train_time:452566ms step_avg:2446.30ms
step:196/500 train_loss:4.8481 train_time:455082ms step_avg:2446.68ms
step:197/500 train_loss:4.9072 train_time:457353ms step_avg:2445.74ms
step:198/500 train_loss:4.7441 train_time:459306ms step_avg:2443.12ms
step:199/500 train_loss:4.9698 train_time:461813ms step_avg:2443.46ms
step:200/500 train_loss:4.8835 train_time:464344ms step_avg:2443.92ms
step:201/500 train_loss:5.6702 train_time:466914ms step_avg:2444.57ms
step:202/500 train_loss:5.0764 train_time:469145ms step_avg:2443.46ms
step:203/500 train_loss:4.8451 train_time:471744ms step_avg:2444.27ms
step:204/500 train_loss:4.7943 train_time:474272ms step_avg:2444.70ms
step:205/500 train_loss:4.8037 train_time:476786ms step_avg:2445.06ms
step:206/500 train_loss:4.7795 train_time:479102ms step_avg:2444.40ms
step:207/500 train_loss:4.8634 train_time:481620ms step_avg:2444.77ms
step:208/500 train_loss:4.7281 train_time:484189ms step_avg:2445.40ms
step:209/500 train_loss:4.8455 train_time:486590ms step_avg:2445.18ms
step:210/500 train_loss:4.7256 train_time:489197ms step_avg:2445.99ms
step:211/500 train_loss:4.7903 train_time:491661ms step_avg:2446.07ms
step:212/500 train_loss:4.7970 train_time:494207ms step_avg:2446.57ms
step:213/500 train_loss:4.7537 train_time:496725ms step_avg:2446.92ms
step:214/500 train_loss:4.7658 train_time:499245ms step_avg:2447.28ms
step:215/500 train_loss:4.8417 train_time:501744ms step_avg:2447.53ms
step:216/500 train_loss:4.7506 train_time:503910ms step_avg:2446.17ms
step:217/500 train_loss:4.7941 train_time:506422ms step_avg:2446.48ms
step:218/500 train_loss:4.7573 train_time:508683ms step_avg:2445.59ms
step:219/500 train_loss:4.6257 train_time:510950ms step_avg:2444.73ms
step:220/500 train_loss:4.7758 train_time:513465ms step_avg:2445.07ms
step:221/500 train_loss:4.8034 train_time:515976ms step_avg:2445.39ms
step:222/500 train_loss:4.7982 train_time:518443ms step_avg:2445.49ms
step:223/500 train_loss:4.7605 train_time:520354ms step_avg:2442.98ms
step:224/500 train_loss:4.7146 train_time:522964ms step_avg:2443.76ms
step:225/500 train_loss:4.7819 train_time:525516ms step_avg:2444.26ms
step:226/500 train_loss:4.6780 train_time:528090ms step_avg:2444.86ms
step:227/500 train_loss:4.7074 train_time:530617ms step_avg:2445.24ms
step:228/500 train_loss:4.6750 train_time:533158ms step_avg:2445.68ms
step:229/500 train_loss:4.8373 train_time:535671ms step_avg:2445.99ms
step:230/500 train_loss:4.7585 train_time:538176ms step_avg:2446.26ms
step:231/500 train_loss:4.6796 train_time:540221ms step_avg:2444.44ms
step:232/500 train_loss:4.8446 train_time:542726ms step_avg:2444.71ms
step:233/500 train_loss:4.6480 train_time:545250ms step_avg:2445.07ms
step:234/500 train_loss:4.7093 train_time:547769ms step_avg:2445.40ms
step:235/500 train_loss:4.8336 train_time:550278ms step_avg:2445.68ms
step:236/500 train_loss:4.7438 train_time:552786ms step_avg:2445.96ms
step:237/500 train_loss:4.8576 train_time:555303ms step_avg:2446.27ms
step:238/500 train_loss:4.7781 train_time:557822ms step_avg:2446.59ms
step:239/500 train_loss:4.7312 train_time:560337ms step_avg:2446.89ms
step:240/500 train_loss:4.7117 train_time:562831ms step_avg:2447.09ms
step:241/500 train_loss:5.0041 train_time:565142ms step_avg:2446.50ms
step:242/500 train_loss:4.7044 train_time:567650ms step_avg:2446.77ms
step:243/500 train_loss:5.0347 train_time:569536ms step_avg:2444.36ms
step:244/500 train_loss:4.6413 train_time:572069ms step_avg:2444.74ms
step:245/500 train_loss:4.7047 train_time:574333ms step_avg:2443.97ms
step:246/500 train_loss:4.7345 train_time:576922ms step_avg:2444.58ms
step:247/500 train_loss:4.6789 train_time:579200ms step_avg:2443.88ms
step:248/500 train_loss:4.6873 train_time:581754ms step_avg:2444.35ms
step:249/500 train_loss:4.5551 train_time:584308ms step_avg:2444.80ms
step:250/500 train_loss:4.7519 train_time:586909ms step_avg:2445.45ms
step:251/500 train_loss:4.6706 train_time:589498ms step_avg:2446.05ms
step:252/500 train_loss:4.5315 train_time:591707ms step_avg:2445.07ms
step:253/500 train_loss:4.5352 train_time:594225ms step_avg:2445.37ms
step:254/500 train_loss:4.6344 train_time:596641ms step_avg:2445.25ms
step:255/500 train_loss:4.6162 train_time:599184ms step_avg:2445.65ms
step:256/500 train_loss:4.5474 train_time:601700ms step_avg:2445.94ms
step:257/500 train_loss:4.6436 train_time:604054ms step_avg:2445.56ms
step:258/500 train_loss:4.7914 train_time:606614ms step_avg:2446.03ms
step:259/500 train_loss:4.5483 train_time:609185ms step_avg:2446.53ms
step:260/500 train_loss:4.6453 train_time:611715ms step_avg:2446.86ms
step:261/500 train_loss:4.5928 train_time:613942ms step_avg:2445.99ms
step:262/500 train_loss:4.5888 train_time:616529ms step_avg:2446.54ms
step:263/500 train_loss:4.6878 train_time:618875ms step_avg:2446.15ms
step:264/500 train_loss:4.6897 train_time:621032ms step_avg:2445.01ms
step:265/500 train_loss:5.0792 train_time:623550ms step_avg:2445.30ms
step:266/500 train_loss:4.6559 train_time:626023ms step_avg:2445.40ms
step:267/500 train_loss:4.7792 train_time:628572ms step_avg:2445.81ms
step:268/500 train_loss:4.5643 train_time:631110ms step_avg:2446.16ms
step:269/500 train_loss:4.5273 train_time:633414ms step_avg:2445.61ms
step:270/500 train_loss:4.6716 train_time:635922ms step_avg:2445.85ms
step:271/500 train_loss:4.7291 train_time:638440ms step_avg:2446.13ms
step:272/500 train_loss:4.6327 train_time:640962ms step_avg:2446.42ms
step:273/500 train_loss:4.5269 train_time:643550ms step_avg:2446.96ms
step:274/500 train_loss:4.7381 train_time:646137ms step_avg:2447.49ms
step:275/500 train_loss:4.5276 train_time:648576ms step_avg:2447.46ms
step:276/500 train_loss:4.6539 train_time:651092ms step_avg:2447.72ms
step:277/500 train_loss:4.6771 train_time:653450ms step_avg:2447.38ms
step:278/500 train_loss:4.4651 train_time:655947ms step_avg:2447.57ms
step:279/500 train_loss:4.7747 train_time:658466ms step_avg:2447.83ms
step:280/500 train_loss:4.6029 train_time:660979ms step_avg:2448.07ms
step:281/500 train_loss:4.5457 train_time:663547ms step_avg:2448.51ms
step:282/500 train_loss:4.5847 train_time:666146ms step_avg:2449.07ms
step:283/500 train_loss:4.7225 train_time:668672ms step_avg:2449.35ms
step:284/500 train_loss:4.5579 train_time:671197ms step_avg:2449.63ms
step:285/500 train_loss:4.5046 train_time:673713ms step_avg:2449.86ms
step:286/500 train_loss:4.6516 train_time:675981ms step_avg:2449.21ms
step:287/500 train_loss:4.5685 train_time:678536ms step_avg:2449.59ms
step:288/500 train_loss:4.5927 train_time:680577ms step_avg:2448.12ms
step:289/500 train_loss:4.6911 train_time:683092ms step_avg:2448.36ms
step:290/500 train_loss:4.6541 train_time:685572ms step_avg:2448.47ms
step:291/500 train_loss:4.4866 train_time:687780ms step_avg:2447.62ms
step:292/500 train_loss:4.4234 train_time:690045ms step_avg:2446.97ms
step:293/500 train_loss:4.6545 train_time:692555ms step_avg:2447.19ms
step:294/500 train_loss:4.4459 train_time:695070ms step_avg:2447.43ms
step:295/500 train_loss:4.6758 train_time:697573ms step_avg:2447.63ms
step:296/500 train_loss:4.6697 train_time:700111ms step_avg:2447.94ms
step:297/500 train_loss:4.4638 train_time:702651ms step_avg:2448.26ms
step:298/500 train_loss:4.6304 train_time:705182ms step_avg:2448.55ms
step:299/500 train_loss:4.4876 train_time:707462ms step_avg:2447.97ms
step:300/500 train_loss:4.7303 train_time:710024ms step_avg:2448.36ms
step:301/500 train_loss:4.4774 train_time:712572ms step_avg:2448.70ms
step:302/500 train_loss:4.5421 train_time:715130ms step_avg:2449.08ms
step:303/500 train_loss:4.5155 train_time:717229ms step_avg:2447.88ms
step:304/500 train_loss:4.5470 train_time:719748ms step_avg:2448.12ms
step:305/500 train_loss:4.5268 train_time:722294ms step_avg:2448.45ms
step:306/500 train_loss:4.6161 train_time:724817ms step_avg:2448.71ms
step:307/500 train_loss:4.5415 train_time:727045ms step_avg:2447.96ms
step:308/500 train_loss:4.4628 train_time:729768ms step_avg:2448.89ms
step:309/500 train_loss:4.6502 train_time:731678ms step_avg:2447.08ms
step:310/500 train_loss:4.4812 train_time:734190ms step_avg:2447.30ms
step:311/500 train_loss:4.5320 train_time:736562ms step_avg:2447.05ms
step:312/500 train_loss:4.5323 train_time:739013ms step_avg:2447.06ms
step:313/500 train_loss:4.4234 train_time:741467ms step_avg:2447.08ms
step:314/500 train_loss:4.4668 train_time:743657ms step_avg:2446.24ms
step:315/500 train_loss:4.5729 train_time:746204ms step_avg:2446.57ms
step:316/500 train_loss:4.2436 train_time:748742ms step_avg:2446.87ms
step:317/500 train_loss:4.5153 train_time:751280ms step_avg:2447.17ms
step:318/500 train_loss:4.4939 train_time:753774ms step_avg:2447.32ms
step:319/500 train_loss:4.4024 train_time:756125ms step_avg:2447.01ms
step:320/500 train_loss:4.4369 train_time:758661ms step_avg:2447.29ms
step:321/500 train_loss:4.4510 train_time:761209ms step_avg:2447.62ms
step:322/500 train_loss:4.5112 train_time:763743ms step_avg:2447.89ms
step:323/500 train_loss:4.3893 train_time:766225ms step_avg:2448.00ms
step:324/500 train_loss:4.4881 train_time:768460ms step_avg:2447.32ms
step:325/500 train_loss:4.4690 train_time:770982ms step_avg:2447.56ms
step:326/500 train_loss:4.4409 train_time:773499ms step_avg:2447.78ms
step:327/500 train_loss:4.4587 train_time:775692ms step_avg:2446.98ms
step:328/500 train_loss:4.6320 train_time:778225ms step_avg:2447.25ms
step:329/500 train_loss:4.5406 train_time:780738ms step_avg:2447.45ms
step:330/500 train_loss:4.4962 train_time:783253ms step_avg:2447.67ms
step:331/500 train_loss:4.4156 train_time:785780ms step_avg:2447.91ms
step:332/500 train_loss:5.0136 train_time:788315ms step_avg:2448.18ms
step:333/500 train_loss:4.3974 train_time:790487ms step_avg:2447.33ms
step:334/500 train_loss:4.3391 train_time:792908ms step_avg:2447.25ms
step:335/500 train_loss:4.4756 train_time:795512ms step_avg:2447.73ms
step:336/500 train_loss:4.5036 train_time:797826ms step_avg:2447.32ms
step:337/500 train_loss:4.4612 train_time:800073ms step_avg:2446.71ms
step:338/500 train_loss:4.4702 train_time:802110ms step_avg:2445.46ms
step:339/500 train_loss:4.4677 train_time:804709ms step_avg:2445.92ms
step:340/500 train_loss:4.3934 train_time:807256ms step_avg:2446.23ms
step:341/500 train_loss:4.3483 train_time:809788ms step_avg:2446.49ms
step:342/500 train_loss:4.3977 train_time:811939ms step_avg:2445.60ms
step:343/500 train_loss:4.3858 train_time:814462ms step_avg:2445.83ms
step:344/500 train_loss:4.5335 train_time:816984ms step_avg:2446.06ms
step:345/500 train_loss:4.4254 train_time:819490ms step_avg:2446.24ms
step:346/500 train_loss:4.3886 train_time:821830ms step_avg:2445.92ms
step:347/500 train_loss:4.4641 train_time:824393ms step_avg:2446.27ms
step:348/500 train_loss:4.3529 train_time:826925ms step_avg:2446.52ms
step:349/500 train_loss:4.4126 train_time:829279ms step_avg:2446.25ms
step:350/500 train_loss:4.4353 train_time:831855ms step_avg:2446.63ms
step:351/500 train_loss:4.6348 train_time:834429ms step_avg:2447.01ms
step:352/500 train_loss:4.3208 train_time:836976ms step_avg:2447.30ms
step:353/500 train_loss:4.3679 train_time:839496ms step_avg:2447.51ms
step:354/500 train_loss:4.3783 train_time:841713ms step_avg:2446.84ms
step:355/500 train_loss:4.2728 train_time:844063ms step_avg:2446.56ms
step:356/500 train_loss:4.4336 train_time:846574ms step_avg:2446.75ms
step:357/500 train_loss:4.3276 train_time:849093ms step_avg:2446.95ms
step:358/500 train_loss:4.4202 train_time:851075ms step_avg:2445.62ms
step:359/500 train_loss:4.4442 train_time:853246ms step_avg:2444.83ms
step:360/500 train_loss:4.3279 train_time:855763ms step_avg:2445.04ms
step:361/500 train_loss:4.6103 train_time:858247ms step_avg:2445.15ms
step:362/500 train_loss:4.5367 train_time:860762ms step_avg:2445.35ms
step:363/500 train_loss:4.4203 train_time:863234ms step_avg:2445.42ms
step:364/500 train_loss:4.3104 train_time:865755ms step_avg:2445.63ms
step:365/500 train_loss:4.4718 train_time:868272ms step_avg:2445.84ms
step:366/500 train_loss:4.3421 train_time:870775ms step_avg:2446.00ms
step:367/500 train_loss:4.3921 train_time:872957ms step_avg:2445.26ms
step:368/500 train_loss:4.3615 train_time:875533ms step_avg:2445.62ms
step:369/500 train_loss:4.4311 train_time:878056ms step_avg:2445.84ms
step:370/500 train_loss:4.3689 train_time:880255ms step_avg:2445.15ms
step:371/500 train_loss:4.2148 train_time:882778ms step_avg:2445.37ms
step:372/500 train_loss:4.3338 train_time:885008ms step_avg:2444.77ms
step:373/500 train_loss:4.3682 train_time:887531ms step_avg:2444.99ms
step:374/500 train_loss:4.3489 train_time:890042ms step_avg:2445.17ms
step:375/500 train_loss:4.4082 train_time:892660ms step_avg:2445.64ms
step:376/500 train_loss:4.3742 train_time:895196ms step_avg:2445.89ms
step:377/500 train_loss:4.2662 train_time:897715ms step_avg:2446.09ms
step:378/500 train_loss:3.7158 train_time:900252ms step_avg:2446.34ms
step:379/500 train_loss:4.1890 train_time:902306ms step_avg:2445.27ms
step:380/500 train_loss:4.2353 train_time:905068ms step_avg:2446.13ms
step:381/500 train_loss:4.3436 train_time:907416ms step_avg:2445.87ms
step:382/500 train_loss:4.3951 train_time:909967ms step_avg:2446.15ms
step:383/500 train_loss:4.3788 train_time:912224ms step_avg:2445.64ms
step:384/500 train_loss:4.2838 train_time:914776ms step_avg:2445.93ms
step:385/500 train_loss:4.3891 train_time:917119ms step_avg:2445.65ms
step:386/500 train_loss:4.2950 train_time:919704ms step_avg:2446.02ms
step:387/500 train_loss:4.4173 train_time:921866ms step_avg:2445.27ms
step:388/500 train_loss:4.6185 train_time:924388ms step_avg:2445.47ms
step:389/500 train_loss:4.3229 train_time:926906ms step_avg:2445.66ms
step:390/500 train_loss:4.2799 train_time:929430ms step_avg:2445.87ms
step:391/500 train_loss:4.4094 train_time:931952ms step_avg:2446.07ms
step:392/500 train_loss:4.3297 train_time:934477ms step_avg:2446.27ms
step:393/500 train_loss:4.4336 train_time:936990ms step_avg:2446.45ms
step:394/500 train_loss:4.2558 train_time:939312ms step_avg:2446.13ms
step:395/500 train_loss:4.3907 train_time:941659ms step_avg:2445.87ms
step:396/500 train_loss:4.1723 train_time:944183ms step_avg:2446.07ms
step:397/500 train_loss:4.3370 train_time:946730ms step_avg:2446.33ms
step:398/500 train_loss:4.4297 train_time:949316ms step_avg:2446.69ms
step:399/500 train_loss:4.3837 train_time:951747ms step_avg:2446.65ms
step:400/500 train_loss:4.3004 train_time:954267ms step_avg:2446.84ms
step:401/500 train_loss:4.3609 train_time:956783ms step_avg:2447.01ms
step:402/500 train_loss:4.4033 train_time:959328ms step_avg:2447.27ms
step:403/500 train_loss:4.3720 train_time:961751ms step_avg:2447.20ms
step:404/500 train_loss:4.4692 train_time:963869ms step_avg:2446.37ms
step:405/500 train_loss:4.2456 train_time:966388ms step_avg:2446.55ms
step:406/500 train_loss:4.2976 train_time:968899ms step_avg:2446.72ms
step:407/500 train_loss:4.5699 train_time:971271ms step_avg:2446.53ms
step:408/500 train_loss:4.3339 train_time:973802ms step_avg:2446.74ms
step:409/500 train_loss:4.3261 train_time:976318ms step_avg:2446.91ms
step:410/500 train_loss:4.3784 train_time:978832ms step_avg:2447.08ms
step:411/500 train_loss:4.2586 train_time:981349ms step_avg:2447.25ms
step:412/500 train_loss:4.2795 train_time:983861ms step_avg:2447.41ms
step:413/500 train_loss:4.6905 train_time:986395ms step_avg:2447.63ms
step:414/500 train_loss:4.1539 train_time:988917ms step_avg:2447.81ms
step:415/500 train_loss:4.5107 train_time:991426ms step_avg:2447.96ms
step:416/500 train_loss:4.2899 train_time:993810ms step_avg:2447.81ms
step:417/500 train_loss:4.2756 train_time:996290ms step_avg:2447.89ms
step:418/500 train_loss:4.4660 train_time:998889ms step_avg:2448.26ms
step:419/500 train_loss:4.1978 train_time:1001423ms step_avg:2448.47ms
step:420/500 train_loss:4.3008 train_time:1003935ms step_avg:2448.62ms
step:421/500 train_loss:4.2660 train_time:1006521ms step_avg:2448.96ms
step:422/500 train_loss:4.1576 train_time:1009065ms step_avg:2449.19ms
step:423/500 train_loss:4.2680 train_time:1011480ms step_avg:2449.10ms
step:424/500 train_loss:4.3779 train_time:1013322ms step_avg:2447.64ms
step:425/500 train_loss:4.1767 train_time:1015897ms step_avg:2447.94ms
step:426/500 train_loss:4.3314 train_time:1018444ms step_avg:2448.18ms
step:427/500 train_loss:4.2242 train_time:1020981ms step_avg:2448.40ms
step:428/500 train_loss:4.4020 train_time:1023324ms step_avg:2448.14ms
step:429/500 train_loss:4.3500 train_time:1025390ms step_avg:2447.23ms
step:430/500 train_loss:4.2633 train_time:1028019ms step_avg:2447.66ms
step:431/500 train_loss:4.2406 train_time:1030243ms step_avg:2447.13ms
step:432/500 train_loss:4.1892 train_time:1032765ms step_avg:2447.31ms
step:433/500 train_loss:4.2728 train_time:1035284ms step_avg:2447.48ms
step:434/500 train_loss:4.3520 train_time:1037796ms step_avg:2447.63ms
step:435/500 train_loss:4.2766 train_time:1040324ms step_avg:2447.82ms
step:436/500 train_loss:4.3290 train_time:1042870ms step_avg:2448.05ms
step:437/500 train_loss:4.3387 train_time:1045392ms step_avg:2448.23ms
step:438/500 train_loss:4.2133 train_time:1047934ms step_avg:2448.44ms
step:439/500 train_loss:4.2407 train_time:1050237ms step_avg:2448.11ms
step:440/500 train_loss:4.2092 train_time:1052752ms step_avg:2448.26ms
step:441/500 train_loss:4.3929 train_time:1054965ms step_avg:2447.71ms
step:442/500 train_loss:4.2935 train_time:1057531ms step_avg:2447.99ms
step:443/500 train_loss:4.2683 train_time:1060074ms step_avg:2448.21ms
step:444/500 train_loss:4.1653 train_time:1062531ms step_avg:2448.23ms
step:445/500 train_loss:4.4215 train_time:1065151ms step_avg:2448.62ms
step:446/500 train_loss:4.3414 train_time:1067648ms step_avg:2448.73ms
step:447/500 train_loss:4.3480 train_time:1070209ms step_avg:2448.99ms
step:448/500 train_loss:4.2575 train_time:1072255ms step_avg:2448.07ms
step:449/500 train_loss:4.3486 train_time:1074776ms step_avg:2448.24ms
step:450/500 train_loss:4.1828 train_time:1077292ms step_avg:2448.39ms
step:451/500 train_loss:4.2208 train_time:1079841ms step_avg:2448.62ms
step:452/500 train_loss:4.1124 train_time:1082118ms step_avg:2448.23ms
step:453/500 train_loss:4.2148 train_time:1084650ms step_avg:2448.42ms
step:454/500 train_loss:4.1932 train_time:1087162ms step_avg:2448.56ms
step:455/500 train_loss:4.1676 train_time:1089685ms step_avg:2448.73ms
step:456/500 train_loss:4.3724 train_time:1092105ms step_avg:2448.67ms
step:457/500 train_loss:4.2310 train_time:1094618ms step_avg:2448.81ms
step:458/500 train_loss:4.3125 train_time:1097197ms step_avg:2449.10ms
step:459/500 train_loss:4.3522 train_time:1099752ms step_avg:2449.34ms
step:460/500 train_loss:4.1500 train_time:1102278ms step_avg:2449.51ms
step:461/500 train_loss:4.3256 train_time:1104892ms step_avg:2449.87ms
step:462/500 train_loss:4.2259 train_time:1107142ms step_avg:2449.43ms
step:463/500 train_loss:4.2056 train_time:1109678ms step_avg:2449.62ms
step:464/500 train_loss:4.3009 train_time:1112228ms step_avg:2449.84ms
step:465/500 train_loss:4.2351 train_time:1114739ms step_avg:2449.98ms
step:466/500 train_loss:4.2311 train_time:1117084ms step_avg:2449.75ms
step:467/500 train_loss:4.3578 train_time:1119281ms step_avg:2449.19ms
step:468/500 train_loss:4.3690 train_time:1121806ms step_avg:2449.36ms
step:469/500 train_loss:4.3280 train_time:1123855ms step_avg:2448.49ms
step:470/500 train_loss:4.2345 train_time:1126363ms step_avg:2448.62ms
step:471/500 train_loss:4.3250 train_time:1128880ms step_avg:2448.76ms
step:472/500 train_loss:4.3652 train_time:1131272ms step_avg:2448.64ms
step:473/500 train_loss:4.2747 train_time:1133687ms step_avg:2448.57ms
step:474/500 train_loss:4.2505 train_time:1136247ms step_avg:2448.81ms
step:475/500 train_loss:4.1230 train_time:1138768ms step_avg:2448.96ms
step:476/500 train_loss:4.5514 train_time:1141286ms step_avg:2449.11ms
step:477/500 train_loss:4.3060 train_time:1143799ms step_avg:2449.25ms
step:478/500 train_loss:4.1132 train_time:1146318ms step_avg:2449.40ms
step:479/500 train_loss:4.3120 train_time:1148917ms step_avg:2449.72ms
step:480/500 train_loss:4.2933 train_time:1151426ms step_avg:2449.84ms
step:481/500 train_loss:4.4240 train_time:1153928ms step_avg:2449.95ms
step:482/500 train_loss:4.2478 train_time:1156496ms step_avg:2450.20ms
step:483/500 train_loss:4.0642 train_time:1159067ms step_avg:2450.46ms
step:484/500 train_loss:4.3404 train_time:1161255ms step_avg:2449.90ms
step:485/500 train_loss:4.1851 train_time:1163788ms step_avg:2450.08ms
step:486/500 train_loss:4.2128 train_time:1166295ms step_avg:2450.20ms
step:487/500 train_loss:4.1630 train_time:1168267ms step_avg:2449.20ms
step:488/500 train_loss:4.1861 train_time:1170833ms step_avg:2449.44ms
step:489/500 train_loss:4.3960 train_time:1173354ms step_avg:2449.59ms
step:490/500 train_loss:4.2462 train_time:1175868ms step_avg:2449.73ms
step:491/500 train_loss:4.1474 train_time:1178402ms step_avg:2449.90ms
step:492/500 train_loss:4.1599 train_time:1180933ms step_avg:2450.07ms
step:493/500 train_loss:4.2688 train_time:1183046ms step_avg:2449.37ms
step:494/500 train_loss:4.1124 train_time:1185209ms step_avg:2448.78ms
step:495/500 train_loss:4.2611 train_time:1187421ms step_avg:2448.29ms
step:496/500 train_loss:4.1808 train_time:1189945ms step_avg:2448.45ms
step:497/500 train_loss:4.1237 train_time:1192298ms step_avg:2448.25ms
step:498/500 train_loss:4.2718 train_time:1194530ms step_avg:2447.81ms
step:499/500 train_loss:4.3581 train_time:1197118ms step_avg:2448.09ms
step:500/500 train_loss:4.4090 train_time:1199314ms step_avg:2447.58ms
step:500/500 val_loss:4.2543 train_time:1199319ms step_avg:2447.59ms


==============================================
Final Results for run_id: a6b2190b-63a6-48da-a6c5-6c13856ff61c, expansion_factor: 4.0, final_val_loss: 4.2543158531188965, training_time_ms: 1199318.6025619507, peak_memory_mib: 31028
==============================================
