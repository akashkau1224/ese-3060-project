====================================================================================================
# Training a model for 500 iterations and seeing how the function, validation loss, and training time change over time (every 50 epochs)
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
import math

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, int(config.expansion_factor * config.n_embd), bias=False)
        self.c_proj  = nn.Linear(int(config.expansion_factor * config.n_embd), config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6
    n_embd : int = 768
    expansion_factor : int = 4

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total
        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = '../fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = '../fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # model hyperparams
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6
    n_embd : int = 768

    expansion_factor : int = 4


def run_training_test(args: Hyperparameters, ddp_rank: int, ddp_local_rank: int, ddp_world_size: int,
                      master_process: bool, run_id: str = None, should_log: bool = True, intermediate_log: str = None):
    """
    Run a complete training and testing cycle.

    Args:
        args: Hyperparameters object containing all training configuration
        ddp_rank: Distributed data parallel rank
        ddp_local_rank: Local rank for device assignment
        ddp_world_size: Total number of processes
        master_process: Whether this is the master process (for logging)
        run_id: Optional run ID for logging. If None, generates a new UUID.

    Returns:
        dict: Results containing final validation loss, training time, peak memory, etc.
    """
    # convenience variables
    B, T = args.device_batch_size, args.sequence_length
    # calculate the number of steps to take in the val loop.
    assert args.val_tokens % (B * T * ddp_world_size) == 0
    val_steps = args.val_tokens // (B * T * ddp_world_size)
    # calculate the steps of gradient accumulation required to attain the desired global batch size.
    assert args.batch_size % (B * ddp_world_size) == 0
    train_accumulation_steps = args.batch_size // (B * ddp_world_size)

    # load tokens
    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
    val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
    if master_process:
        print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
        print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
    x, y = train_loader.next_batch()

    # Initialize model
    model = GPT(GPTConfig(vocab_size=args.vocab_size, n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd, expansion_factor=args.expansion_factor))
    model = model.cuda()
    if hasattr(config, "coordinate_descent_tuning"):
        config.coordinate_descent_tuning = True # suggested by @Chillee
    model = torch.compile(model)
    # here we wrap model into DDP container
    model = DDP(model, device_ids=[ddp_local_rank])
    raw_model = model.module # always contains the "raw" unwrapped model
    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

    # init the optimizer(s)
    optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                                   weight_decay=args.weight_decay, fused=True)
    optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
    optimizers = [optimizer1, optimizer2]
    # learning rate decay scheduler (linear warmup and warmdown)
    def get_lr(it):
        assert it <= args.num_iterations
        # 1) linear warmup for warmup_iters steps
        if it < args.warmup_iters:
            return (it+1) / args.warmup_iters
        # 2) constant lr for a while
        elif it < args.num_iterations - args.warmdown_iters:
            return 1.0
        # 3) linear warmdown
        else:
            decay_ratio = (args.num_iterations - it) / args.warmdown_iters
            return decay_ratio
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

    # begin logging
    logfile = None
    if run_id is None:
        run_id = str(uuid.uuid4()) if master_process else None
    if master_process and should_log:
        logdir = 'logs/%s/' % run_id
        os.makedirs(logdir, exist_ok=True)
        logfile = 'logs/%s.txt' % run_id
        # create the log file
        with open(logfile, "w") as f:
            # begin the log by printing this file (the Python code)
            f.write('='*100 + '\n')
            f.write(code)
            f.write('='*100 + '\n')
            # log information about the hardware/software environment this is running on
            # and print the full `nvidia-smi` to file
            f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
            import subprocess
            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            f.write(f'{result.stdout}\n')
            f.write('='*100 + '\n')

    training_time_ms = 0
    final_val_loss = None
    # start the clock
    torch.cuda.synchronize()
    t0 = time.time()
    # begin training
    train_loader.reset()
    for step in range(args.num_iterations + 1):
        last_step = (step == args.num_iterations)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.time()
        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val
        # once in a while evaluate the validation dataset
        if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.time() - t0)
            # run validation batches
            model.eval()
            val_loader.reset()
            val_loss = 0.0
            for _ in range(val_steps):
                x_val, y_val = val_loader.next_batch()
                with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                    _, loss = model(x_val, y_val, return_logits=False)
                    val_loss += loss.detach()
                    del loss
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            val_loss /= val_steps
            final_val_loss = val_loss.item() if last_step else final_val_loss
            # log val loss to console and to logfile
            if master_process and should_log == True:
                print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
                if last_step:
                    with open(logfile, "a") as f:
                        f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
            if master_process and intermediate_log:
                with open(intermediate_log, "a") as f:
                    f.write(f"Step {step} of {args.num_iterations} completed: training_time_ms={training_time_ms:.0f}ms\n")
                    f.write(f"Val loss: {val_loss:.4f}\n")
                    f.write(f"Function value: {function(val_loss, training_time_ms):.4f}\n")
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.time()

        if master_process and should_log and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.time() - t0)
            # save the state of the training process
            log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.time()

        # bit confusing: we want to make sure to eval on 0th iteration
        # but also after the very last iteration. so we loop for step <= num_iterations
        # instead of just < num_iterations (one extra due to <=), only to do
        # the validation/sampling one last time, and then we break right here as we're done.
        if last_step:
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        model.train()
        for i in range(1, train_accumulation_steps+1):
            # forward pass
            with ctx:
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
            # advance the dataset for the next batch
            x, y = train_loader.next_batch()
            # backward pass
            if i < train_accumulation_steps:
                with model.no_sync(): # there's no need to sync gradients every accumulation step
                    loss.backward()
            else:
                loss.backward() # just sync on the last step
        for p in model.parameters():
            p.grad /= train_accumulation_steps
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # --------------- TRAINING SECTION END -------------------
        # everything that follows now is just diagnostics, prints, logging, etc.

        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
        if master_process and should_log:
            approx_time = training_time_ms + 1000 * (time.time() - t0)
            print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
            with open(logfile, "a") as f:
                f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

    # Collect results
    peak_memory_mib = torch.cuda.max_memory_allocated() // 1024 // 1024
    if master_process and should_log:
        print(f"peak memory consumption: {peak_memory_mib} MiB")

    results = {
        'final_val_loss': final_val_loss,
        'training_time_ms': training_time_ms,
        'peak_memory_mib': peak_memory_mib,
        'run_id': run_id if master_process else None,
        'expansion_factor': args.expansion_factor
    }

    if master_process and should_log:
        with open(logfile, "a") as f:
            f.write(f"\n\n==============================================\nFinal Results for run_id: {run_id}, expansion_factor: {args.expansion_factor}, final_val_loss: {final_val_loss}, training_time_ms: {training_time_ms}, peak_memory_mib: {peak_memory_mib}\n==============================================\n\n")

    return results

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

def function(validation_loss: float, training_time: float) -> float:
    return (0.965 * math.log(validation_loss) + 0.035 * math.log(training_time))
# Single test
# args = Hyperparameters()
# results = run_training_test(args, ddp_rank, ddp_local_rank, ddp_world_size, master_process)
# Not logging individual test results for initial testing

long_test_log = "./logs/function_testing.txt"
if master_process:
    with open(long_test_log, "w") as f:
        f.write(f"More graunuar testing with custom functionlog\n")
        f.write(f"==============================================\n")
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n\n')
        f.write(f"==============================================\n")
        f.write(f"Testing expansion factors\n")
        f.write(f"==============================================\n")
# Extreme examples to test timing of training a model with different expansion factors
for i in [3.1, 3.5, 3.8, 4.0]:
    if master_process:
        with open(long_test_log, "a") as f:
            f.write(f"==============================================\n")
            f.write(f"Testing expansion factor {i} with 500 iterations\n")
            f.write(f"==============================================\n")
    args = Hyperparameters(expansion_factor=i, num_iterations=500, val_loss_every=50)
    results = run_training_test(args, ddp_rank, ddp_local_rank, ddp_world_size, master_process, should_log=True, intermediate_log=long_test_log)
    print(f"Test with expansion factor {i} completed: final_val_loss={results['final_val_loss']:.4f}, training_time={results['training_time_ms']:.0f}ms")

    function_value = function(results['final_val_loss'], results['training_time_ms'])
    print(f"Function value for expansion factor {i}: {function_value:.4f}")
    if master_process:
        with open(long_test_log, "a") as f:
            f.write(f"==============================================\n")
            f.write(f"Test with expansion factor {i} completed: final_val_loss={results['final_val_loss']:.4f}, training_time={results['training_time_ms']:.0f}ms\n")
            f.write(f"Function value for expansion factor {i}: {function_value:.4f}\n")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  2 20:41:08 2025
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40                     On  |   00000000:01:00.0 Off |                    0 |
| N/A   41C    P0             90W /  300W |   36047MiB /  46068MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA L40                     On  |   00000000:A1:00.0 Off |                    0 |
| N/A   38C    P0             89W /  300W |   36047MiB /  46068MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA L40                     On  |   00000000:C1:00.0 Off |                    0 |
| N/A   39C    P0             87W /  300W |   36047MiB /  46068MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA L40                     On  |   00000000:E1:00.0 Off |                    0 |
| N/A   40C    P0             86W /  300W |   36047MiB /  46068MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:1/500 train_loss:15.9953 train_time:82280ms step_avg:nanms
step:2/500 train_loss:9.5841 train_time:84668ms step_avg:nanms
step:3/500 train_loss:8.8801 train_time:86954ms step_avg:nanms
step:4/500 train_loss:8.2920 train_time:89259ms step_avg:nanms
step:5/500 train_loss:7.9976 train_time:91644ms step_avg:nanms
step:6/500 train_loss:7.6306 train_time:94036ms step_avg:nanms
step:7/500 train_loss:7.8235 train_time:96426ms step_avg:nanms
step:8/500 train_loss:7.4995 train_time:98451ms step_avg:nanms
step:9/500 train_loss:7.2937 train_time:100837ms step_avg:nanms
step:10/500 train_loss:7.2438 train_time:103256ms step_avg:nanms
step:11/500 train_loss:7.1405 train_time:2395ms step_avg:nanms
step:12/500 train_loss:6.9711 train_time:4649ms step_avg:nanms
step:13/500 train_loss:6.9608 train_time:7040ms step_avg:2346.72ms
step:14/500 train_loss:6.9031 train_time:9347ms step_avg:2336.69ms
step:15/500 train_loss:6.8098 train_time:11388ms step_avg:2277.69ms
step:16/500 train_loss:6.8047 train_time:13773ms step_avg:2295.49ms
step:17/500 train_loss:6.8354 train_time:16078ms step_avg:2296.90ms
step:18/500 train_loss:6.6556 train_time:18524ms step_avg:2315.53ms
step:19/500 train_loss:6.6501 train_time:20860ms step_avg:2317.79ms
step:20/500 train_loss:6.3234 train_time:23284ms step_avg:2328.38ms
step:21/500 train_loss:6.7226 train_time:25494ms step_avg:2317.61ms
step:22/500 train_loss:6.9510 train_time:27804ms step_avg:2316.97ms
step:23/500 train_loss:6.5846 train_time:29786ms step_avg:2291.21ms
step:24/500 train_loss:6.6905 train_time:32185ms step_avg:2298.95ms
step:25/500 train_loss:6.4141 train_time:34582ms step_avg:2305.47ms
step:26/500 train_loss:6.3320 train_time:36741ms step_avg:2296.32ms
step:27/500 train_loss:6.5023 train_time:38829ms step_avg:2284.08ms
step:28/500 train_loss:6.1518 train_time:41104ms step_avg:2283.54ms
step:29/500 train_loss:6.4347 train_time:43480ms step_avg:2288.42ms
step:30/500 train_loss:6.2843 train_time:45930ms step_avg:2296.52ms
step:31/500 train_loss:6.2407 train_time:48341ms step_avg:2301.97ms
step:32/500 train_loss:6.0552 train_time:50759ms step_avg:2307.21ms
step:33/500 train_loss:6.4163 train_time:53235ms step_avg:2314.55ms
step:34/500 train_loss:6.3100 train_time:55340ms step_avg:2305.82ms
step:35/500 train_loss:6.4824 train_time:57828ms step_avg:2313.12ms
step:36/500 train_loss:6.4022 train_time:60233ms step_avg:2316.67ms
step:37/500 train_loss:6.2718 train_time:62632ms step_avg:2319.71ms
step:38/500 train_loss:6.1665 train_time:65038ms step_avg:2322.80ms
step:39/500 train_loss:6.2385 train_time:67433ms step_avg:2325.27ms
step:40/500 train_loss:6.1278 train_time:69807ms step_avg:2326.91ms
step:41/500 train_loss:6.1708 train_time:71659ms step_avg:2311.58ms
step:42/500 train_loss:6.0357 train_time:74090ms step_avg:2315.33ms
step:43/500 train_loss:6.1182 train_time:76565ms step_avg:2320.16ms
step:44/500 train_loss:6.1124 train_time:78810ms step_avg:2317.93ms
step:45/500 train_loss:6.2925 train_time:81028ms step_avg:2315.08ms
step:46/500 train_loss:6.0981 train_time:83428ms step_avg:2317.45ms
step:47/500 train_loss:5.9337 train_time:85823ms step_avg:2319.53ms
step:48/500 train_loss:6.1854 train_time:88226ms step_avg:2321.73ms
step:49/500 train_loss:6.0514 train_time:90232ms step_avg:2313.64ms
step:50/500 train_loss:6.2094 train_time:92630ms step_avg:2315.75ms
step:51/500 train_loss:6.0711 train_time:94944ms step_avg:2315.71ms
step:52/500 train_loss:5.9116 train_time:97433ms step_avg:2319.82ms
step:53/500 train_loss:6.0523 train_time:99864ms step_avg:2322.42ms
step:54/500 train_loss:5.9684 train_time:102315ms step_avg:2325.35ms
step:55/500 train_loss:6.2734 train_time:104538ms step_avg:2323.06ms
step:56/500 train_loss:5.9555 train_time:107026ms step_avg:2326.65ms
step:57/500 train_loss:5.8444 train_time:109548ms step_avg:2330.82ms
step:58/500 train_loss:6.0057 train_time:111942ms step_avg:2332.12ms
step:59/500 train_loss:5.9325 train_time:114368ms step_avg:2334.04ms
step:60/500 train_loss:6.0404 train_time:116514ms step_avg:2330.27ms
step:61/500 train_loss:5.8281 train_time:118928ms step_avg:2331.92ms
step:62/500 train_loss:5.9232 train_time:121255ms step_avg:2331.83ms
step:63/500 train_loss:5.8878 train_time:123331ms step_avg:2327.00ms
step:64/500 train_loss:6.0426 train_time:125593ms step_avg:2325.80ms
step:65/500 train_loss:5.7228 train_time:128044ms step_avg:2328.07ms
step:66/500 train_loss:5.8874 train_time:130235ms step_avg:2325.63ms
step:67/500 train_loss:5.7535 train_time:132366ms step_avg:2322.21ms
step:68/500 train_loss:5.9976 train_time:134686ms step_avg:2322.17ms
step:69/500 train_loss:5.6742 train_time:137100ms step_avg:2323.72ms
step:70/500 train_loss:5.6973 train_time:139490ms step_avg:2324.83ms
step:71/500 train_loss:5.8982 train_time:141676ms step_avg:2322.56ms
step:72/500 train_loss:5.8335 train_time:144016ms step_avg:2322.84ms
step:73/500 train_loss:5.7233 train_time:146468ms step_avg:2324.89ms
step:74/500 train_loss:5.8421 train_time:148921ms step_avg:2326.89ms
step:75/500 train_loss:5.8092 train_time:151295ms step_avg:2327.62ms
step:76/500 train_loss:5.7705 train_time:153405ms step_avg:2324.31ms
step:77/500 train_loss:5.8437 train_time:155795ms step_avg:2325.30ms
step:78/500 train_loss:5.8696 train_time:158173ms step_avg:2326.08ms
step:79/500 train_loss:5.7347 train_time:160561ms step_avg:2326.97ms
step:80/500 train_loss:5.8281 train_time:162947ms step_avg:2327.81ms
step:81/500 train_loss:5.5864 train_time:165342ms step_avg:2328.76ms
step:82/500 train_loss:5.7659 train_time:167705ms step_avg:2329.23ms
step:83/500 train_loss:5.7322 train_time:170101ms step_avg:2330.14ms
step:84/500 train_loss:5.6881 train_time:172500ms step_avg:2331.08ms
step:85/500 train_loss:5.5555 train_time:174936ms step_avg:2332.48ms
step:86/500 train_loss:5.7694 train_time:177361ms step_avg:2333.70ms
step:87/500 train_loss:5.6612 train_time:179760ms step_avg:2334.54ms
step:88/500 train_loss:5.7227 train_time:181932ms step_avg:2332.46ms
step:89/500 train_loss:5.7094 train_time:184065ms step_avg:2329.94ms
step:90/500 train_loss:5.6259 train_time:186502ms step_avg:2331.28ms
step:91/500 train_loss:5.6247 train_time:188940ms step_avg:2332.60ms
step:92/500 train_loss:5.7129 train_time:191333ms step_avg:2333.33ms
step:93/500 train_loss:5.5738 train_time:193405ms step_avg:2330.19ms
step:94/500 train_loss:5.5610 train_time:195834ms step_avg:2331.36ms
step:95/500 train_loss:5.5803 train_time:198219ms step_avg:2331.99ms
step:96/500 train_loss:5.4957 train_time:200571ms step_avg:2332.23ms
step:97/500 train_loss:5.5703 train_time:202879ms step_avg:2331.94ms
step:98/500 train_loss:5.4876 train_time:205175ms step_avg:2331.53ms
step:99/500 train_loss:5.6155 train_time:207622ms step_avg:2332.83ms
step:100/500 train_loss:5.5761 train_time:210023ms step_avg:2333.59ms
step:101/500 train_loss:5.4904 train_time:212459ms step_avg:2334.72ms
step:102/500 train_loss:5.5827 train_time:214908ms step_avg:2335.95ms
step:103/500 train_loss:5.5469 train_time:217257ms step_avg:2336.09ms
step:104/500 train_loss:5.3705 train_time:219652ms step_avg:2336.73ms
step:105/500 train_loss:5.4802 train_time:222067ms step_avg:2337.55ms
step:106/500 train_loss:5.6965 train_time:224464ms step_avg:2338.16ms
step:107/500 train_loss:5.4715 train_time:226914ms step_avg:2339.32ms
step:108/500 train_loss:5.2232 train_time:229309ms step_avg:2339.88ms
step:109/500 train_loss:5.4352 train_time:231707ms step_avg:2340.47ms
step:110/500 train_loss:5.3996 train_time:233770ms step_avg:2337.70ms
step:111/500 train_loss:5.3749 train_time:236238ms step_avg:2338.99ms
step:112/500 train_loss:5.4805 train_time:238636ms step_avg:2339.57ms
step:113/500 train_loss:5.4111 train_time:241031ms step_avg:2340.11ms
step:114/500 train_loss:5.2641 train_time:243211ms step_avg:2338.57ms
step:115/500 train_loss:5.4364 train_time:245547ms step_avg:2338.54ms
step:116/500 train_loss:5.2896 train_time:248010ms step_avg:2339.71ms
step:117/500 train_loss:5.2899 train_time:250509ms step_avg:2341.21ms
step:118/500 train_loss:5.4006 train_time:252901ms step_avg:2341.68ms
step:119/500 train_loss:5.3966 train_time:255245ms step_avg:2341.70ms
step:120/500 train_loss:5.3165 train_time:257484ms step_avg:2340.76ms
step:121/500 train_loss:5.2147 train_time:259883ms step_avg:2341.29ms
step:122/500 train_loss:5.3064 train_time:262209ms step_avg:2341.15ms
step:123/500 train_loss:5.1740 train_time:264613ms step_avg:2341.71ms
step:124/500 train_loss:5.4696 train_time:267031ms step_avg:2342.38ms
step:125/500 train_loss:5.3264 train_time:269466ms step_avg:2343.18ms
step:126/500 train_loss:5.3012 train_time:271915ms step_avg:2344.09ms
step:127/500 train_loss:5.3692 train_time:274224ms step_avg:2343.80ms
step:128/500 train_loss:5.2216 train_time:276623ms step_avg:2344.27ms
step:129/500 train_loss:5.4904 train_time:278721ms step_avg:2342.19ms
step:130/500 train_loss:5.2869 train_time:280861ms step_avg:2340.50ms
step:131/500 train_loss:5.2832 train_time:282971ms step_avg:2338.60ms
step:132/500 train_loss:5.2177 train_time:285369ms step_avg:2339.09ms
step:133/500 train_loss:5.2603 train_time:287811ms step_avg:2339.93ms
step:134/500 train_loss:5.1964 train_time:290124ms step_avg:2339.71ms
step:135/500 train_loss:5.2556 train_time:292262ms step_avg:2338.09ms
step:136/500 train_loss:5.0664 train_time:294415ms step_avg:2336.62ms
step:137/500 train_loss:5.2243 train_time:296809ms step_avg:2337.08ms
step:138/500 train_loss:5.1889 train_time:299301ms step_avg:2338.29ms
step:139/500 train_loss:5.1928 train_time:301449ms step_avg:2336.81ms
step:140/500 train_loss:5.2371 train_time:303523ms step_avg:2334.79ms
step:141/500 train_loss:5.1420 train_time:305909ms step_avg:2335.19ms
step:142/500 train_loss:5.2300 train_time:308299ms step_avg:2335.60ms
step:143/500 train_loss:5.0319 train_time:310404ms step_avg:2333.87ms
step:144/500 train_loss:5.1878 train_time:312726ms step_avg:2333.78ms
step:145/500 train_loss:5.1275 train_time:314901ms step_avg:2332.60ms
step:146/500 train_loss:5.0480 train_time:317074ms step_avg:2331.42ms
step:147/500 train_loss:5.1593 train_time:319479ms step_avg:2331.96ms
step:148/500 train_loss:5.1283 train_time:321879ms step_avg:2332.45ms
step:149/500 train_loss:5.2047 train_time:324297ms step_avg:2333.07ms
step:150/500 train_loss:5.2072 train_time:326686ms step_avg:2333.47ms
step:151/500 train_loss:5.1265 train_time:329028ms step_avg:2333.53ms
step:152/500 train_loss:5.1064 train_time:331456ms step_avg:2334.19ms
step:153/500 train_loss:5.1809 train_time:333874ms step_avg:2334.79ms
step:154/500 train_loss:5.1154 train_time:336274ms step_avg:2335.23ms
step:155/500 train_loss:5.0982 train_time:338467ms step_avg:2334.26ms
step:156/500 train_loss:5.1073 train_time:340879ms step_avg:2334.79ms
step:157/500 train_loss:5.2406 train_time:343282ms step_avg:2335.25ms
step:158/500 train_loss:5.0210 train_time:345359ms step_avg:2333.51ms
step:159/500 train_loss:5.0742 train_time:347644ms step_avg:2333.18ms
step:160/500 train_loss:4.9409 train_time:350067ms step_avg:2333.78ms
step:161/500 train_loss:5.0852 train_time:352467ms step_avg:2334.22ms
step:162/500 train_loss:5.1228 train_time:354776ms step_avg:2334.05ms
step:163/500 train_loss:5.1119 train_time:357233ms step_avg:2334.86ms
step:164/500 train_loss:4.9393 train_time:359586ms step_avg:2334.98ms
step:165/500 train_loss:5.0510 train_time:361998ms step_avg:2335.47ms
step:166/500 train_loss:5.2070 train_time:364392ms step_avg:2335.85ms
step:167/500 train_loss:4.9834 train_time:366548ms step_avg:2334.70ms
step:168/500 train_loss:5.0677 train_time:368963ms step_avg:2335.21ms
step:169/500 train_loss:4.9324 train_time:371259ms step_avg:2334.96ms
step:170/500 train_loss:4.8797 train_time:373684ms step_avg:2335.52ms
step:171/500 train_loss:4.9874 train_time:376106ms step_avg:2336.06ms
step:172/500 train_loss:4.9522 train_time:378425ms step_avg:2335.96ms
step:173/500 train_loss:5.0125 train_time:380803ms step_avg:2336.22ms
step:174/500 train_loss:5.1549 train_time:383266ms step_avg:2336.98ms
step:175/500 train_loss:5.0413 train_time:385682ms step_avg:2337.47ms
step:176/500 train_loss:4.8720 train_time:387899ms step_avg:2336.74ms
step:177/500 train_loss:4.8574 train_time:390249ms step_avg:2336.82ms
step:178/500 train_loss:4.8934 train_time:392747ms step_avg:2337.78ms
step:179/500 train_loss:4.9420 train_time:395146ms step_avg:2338.14ms
step:180/500 train_loss:4.9279 train_time:397561ms step_avg:2338.59ms
step:181/500 train_loss:5.0374 train_time:399969ms step_avg:2339.00ms
step:182/500 train_loss:4.9263 train_time:402361ms step_avg:2339.31ms
step:183/500 train_loss:4.8498 train_time:404311ms step_avg:2337.06ms
step:184/500 train_loss:4.8877 train_time:406391ms step_avg:2335.58ms
step:185/500 train_loss:5.0162 train_time:408665ms step_avg:2335.23ms
step:186/500 train_loss:4.8899 train_time:411057ms step_avg:2335.55ms
step:187/500 train_loss:5.1429 train_time:413492ms step_avg:2336.11ms
step:188/500 train_loss:4.9215 train_time:415697ms step_avg:2335.37ms
step:189/500 train_loss:4.8454 train_time:418157ms step_avg:2336.07ms
step:190/500 train_loss:5.0045 train_time:422026ms step_avg:2344.59ms
step:191/500 train_loss:5.0355 train_time:424735ms step_avg:2346.60ms
step:192/500 train_loss:4.8647 train_time:427054ms step_avg:2346.45ms
step:193/500 train_loss:4.8971 train_time:429470ms step_avg:2346.83ms
step:194/500 train_loss:4.8870 train_time:431537ms step_avg:2345.31ms
step:195/500 train_loss:4.8589 train_time:433974ms step_avg:2345.80ms
step:196/500 train_loss:4.8610 train_time:436375ms step_avg:2346.10ms
step:197/500 train_loss:4.9121 train_time:438876ms step_avg:2346.93ms
step:198/500 train_loss:4.7559 train_time:441276ms step_avg:2347.21ms
step:199/500 train_loss:4.9820 train_time:443666ms step_avg:2347.44ms
step:200/500 train_loss:4.8874 train_time:446091ms step_avg:2347.85ms
step:201/500 train_loss:5.6809 train_time:448501ms step_avg:2348.17ms
step:202/500 train_loss:5.0867 train_time:450614ms step_avg:2346.95ms
step:203/500 train_loss:4.8499 train_time:453014ms step_avg:2347.22ms
step:204/500 train_loss:4.8062 train_time:455138ms step_avg:2346.07ms
step:205/500 train_loss:4.8113 train_time:457280ms step_avg:2345.02ms
step:206/500 train_loss:4.7894 train_time:459710ms step_avg:2345.46ms
step:207/500 train_loss:4.8799 train_time:461900ms step_avg:2344.67ms
step:208/500 train_loss:4.7397 train_time:464382ms step_avg:2345.36ms
step:209/500 train_loss:4.8581 train_time:466660ms step_avg:2345.02ms
step:210/500 train_loss:4.7339 train_time:469052ms step_avg:2345.26ms
step:211/500 train_loss:4.7983 train_time:471212ms step_avg:2344.34ms
step:212/500 train_loss:4.8091 train_time:473623ms step_avg:2344.67ms
step:213/500 train_loss:4.7602 train_time:476070ms step_avg:2345.17ms
step:214/500 train_loss:4.7821 train_time:478505ms step_avg:2345.61ms
step:215/500 train_loss:4.8486 train_time:480687ms step_avg:2344.82ms
step:216/500 train_loss:4.7547 train_time:482982ms step_avg:2344.57ms
step:217/500 train_loss:4.8061 train_time:485237ms step_avg:2344.14ms
step:218/500 train_loss:4.7696 train_time:487670ms step_avg:2344.57ms
step:219/500 train_loss:4.6304 train_time:490089ms step_avg:2344.92ms
step:220/500 train_loss:4.7809 train_time:492581ms step_avg:2345.62ms
step:221/500 train_loss:4.8180 train_time:494990ms step_avg:2345.92ms
step:222/500 train_loss:4.8109 train_time:497409ms step_avg:2346.27ms
step:223/500 train_loss:4.7695 train_time:499659ms step_avg:2345.82ms
step:224/500 train_loss:4.7209 train_time:502066ms step_avg:2346.10ms
step:225/500 train_loss:4.7846 train_time:504384ms step_avg:2345.97ms
step:226/500 train_loss:4.6893 train_time:506864ms step_avg:2346.59ms
step:227/500 train_loss:4.7149 train_time:509340ms step_avg:2347.19ms
step:228/500 train_loss:4.6911 train_time:511745ms step_avg:2347.45ms
step:229/500 train_loss:4.8453 train_time:514165ms step_avg:2347.79ms
step:230/500 train_loss:4.7681 train_time:516298ms step_avg:2346.81ms
step:231/500 train_loss:4.6992 train_time:518720ms step_avg:2347.15ms
step:232/500 train_loss:4.8603 train_time:520976ms step_avg:2346.74ms
step:233/500 train_loss:4.6585 train_time:523412ms step_avg:2347.14ms
step:234/500 train_loss:4.7143 train_time:525593ms step_avg:2346.40ms
step:235/500 train_loss:4.8497 train_time:527985ms step_avg:2346.60ms
step:236/500 train_loss:4.7534 train_time:530403ms step_avg:2346.92ms
step:237/500 train_loss:4.8738 train_time:532758ms step_avg:2346.95ms
step:238/500 train_loss:4.7831 train_time:535141ms step_avg:2347.11ms
step:239/500 train_loss:4.7352 train_time:537542ms step_avg:2347.35ms
step:240/500 train_loss:4.7235 train_time:539905ms step_avg:2347.41ms
step:241/500 train_loss:5.0180 train_time:542309ms step_avg:2347.66ms
step:242/500 train_loss:4.7188 train_time:544420ms step_avg:2346.64ms
step:243/500 train_loss:5.0434 train_time:546817ms step_avg:2346.86ms
step:244/500 train_loss:4.6503 train_time:549218ms step_avg:2347.08ms
step:245/500 train_loss:4.7173 train_time:551167ms step_avg:2345.39ms
step:246/500 train_loss:4.7423 train_time:553614ms step_avg:2345.82ms
step:247/500 train_loss:4.6869 train_time:555764ms step_avg:2345.00ms
step:248/500 train_loss:4.7051 train_time:558157ms step_avg:2345.20ms
step:249/500 train_loss:4.5674 train_time:560573ms step_avg:2345.50ms
step:250/500 train_loss:4.7628 train_time:562988ms step_avg:2345.78ms
step:251/500 train_loss:4.6872 train_time:565380ms step_avg:2345.98ms
step:252/500 train_loss:4.5428 train_time:567659ms step_avg:2345.70ms
step:253/500 train_loss:4.5441 train_time:570051ms step_avg:2345.89ms
step:254/500 train_loss:4.6480 train_time:572445ms step_avg:2346.09ms
step:255/500 train_loss:4.6295 train_time:574726ms step_avg:2345.82ms
step:256/500 train_loss:4.5565 train_time:576977ms step_avg:2345.43ms
step:257/500 train_loss:4.6581 train_time:579363ms step_avg:2345.60ms
step:258/500 train_loss:4.8019 train_time:581803ms step_avg:2345.98ms
step:259/500 train_loss:4.5606 train_time:584231ms step_avg:2346.31ms
step:260/500 train_loss:4.6619 train_time:586454ms step_avg:2345.82ms
step:261/500 train_loss:4.6002 train_time:588864ms step_avg:2346.07ms
step:262/500 train_loss:4.6014 train_time:591109ms step_avg:2345.67ms
step:263/500 train_loss:4.6966 train_time:593446ms step_avg:2345.64ms
step:264/500 train_loss:4.7012 train_time:595859ms step_avg:2345.90ms
step:265/500 train_loss:5.0858 train_time:597938ms step_avg:2344.85ms
step:266/500 train_loss:4.6657 train_time:600365ms step_avg:2345.18ms
step:267/500 train_loss:4.7951 train_time:602488ms step_avg:2344.31ms
step:268/500 train_loss:4.5803 train_time:604900ms step_avg:2344.57ms
step:269/500 train_loss:4.5376 train_time:607291ms step_avg:2344.75ms
step:270/500 train_loss:4.6836 train_time:609748ms step_avg:2345.18ms
step:271/500 train_loss:4.7389 train_time:611970ms step_avg:2344.71ms
step:272/500 train_loss:4.6467 train_time:614371ms step_avg:2344.93ms
step:273/500 train_loss:4.5396 train_time:616842ms step_avg:2345.41ms
step:274/500 train_loss:4.7495 train_time:619250ms step_avg:2345.64ms
step:275/500 train_loss:4.5250 train_time:621442ms step_avg:2345.06ms
step:276/500 train_loss:4.6656 train_time:623864ms step_avg:2345.35ms
step:277/500 train_loss:4.6953 train_time:626026ms step_avg:2344.67ms
step:278/500 train_loss:4.4797 train_time:628164ms step_avg:2343.90ms
step:279/500 train_loss:4.7900 train_time:630354ms step_avg:2343.32ms
step:280/500 train_loss:4.6114 train_time:632743ms step_avg:2343.49ms
step:281/500 train_loss:4.5544 train_time:635137ms step_avg:2343.68ms
step:282/500 train_loss:4.5925 train_time:637541ms step_avg:2343.90ms
step:283/500 train_loss:4.7278 train_time:639729ms step_avg:2343.33ms
step:284/500 train_loss:4.5695 train_time:642180ms step_avg:2343.72ms
step:285/500 train_loss:4.5127 train_time:644282ms step_avg:2342.84ms
step:286/500 train_loss:4.6709 train_time:646377ms step_avg:2341.94ms
step:287/500 train_loss:4.5791 train_time:648781ms step_avg:2342.17ms
step:288/500 train_loss:4.6100 train_time:651191ms step_avg:2342.41ms
step:289/500 train_loss:4.7035 train_time:653455ms step_avg:2342.13ms
step:290/500 train_loss:4.6681 train_time:655684ms step_avg:2341.73ms
step:291/500 train_loss:4.5008 train_time:657949ms step_avg:2341.46ms
step:292/500 train_loss:4.4364 train_time:660040ms step_avg:2340.57ms
step:293/500 train_loss:4.6690 train_time:662529ms step_avg:2341.09ms
step:294/500 train_loss:4.4617 train_time:664929ms step_avg:2341.30ms
step:295/500 train_loss:4.6803 train_time:667345ms step_avg:2341.56ms
step:296/500 train_loss:4.6764 train_time:669766ms step_avg:2341.84ms
step:297/500 train_loss:4.4799 train_time:671975ms step_avg:2341.38ms
step:298/500 train_loss:4.6462 train_time:674404ms step_avg:2341.68ms
step:299/500 train_loss:4.4967 train_time:676805ms step_avg:2341.88ms
step:300/500 train_loss:4.7390 train_time:679212ms step_avg:2342.11ms
step:301/500 train_loss:4.4908 train_time:681613ms step_avg:2342.31ms
step:302/500 train_loss:4.5496 train_time:684005ms step_avg:2342.48ms
step:303/500 train_loss:4.5285 train_time:686207ms step_avg:2342.00ms
step:304/500 train_loss:4.5562 train_time:688603ms step_avg:2342.19ms
step:305/500 train_loss:4.5325 train_time:690814ms step_avg:2341.74ms
step:306/500 train_loss:4.6294 train_time:693282ms step_avg:2342.17ms
step:307/500 train_loss:4.5589 train_time:695674ms step_avg:2342.34ms
step:308/500 train_loss:4.4877 train_time:698075ms step_avg:2342.53ms
step:309/500 train_loss:4.6619 train_time:700490ms step_avg:2342.78ms
step:310/500 train_loss:4.4975 train_time:702911ms step_avg:2343.04ms
step:311/500 train_loss:4.5460 train_time:705319ms step_avg:2343.25ms
step:312/500 train_loss:4.5522 train_time:707676ms step_avg:2343.30ms
step:313/500 train_loss:4.4299 train_time:710090ms step_avg:2343.53ms
step:314/500 train_loss:4.4777 train_time:712299ms step_avg:2343.09ms
step:315/500 train_loss:4.5947 train_time:714703ms step_avg:2343.29ms
step:316/500 train_loss:4.2550 train_time:717111ms step_avg:2343.50ms
step:317/500 train_loss:4.5375 train_time:719501ms step_avg:2343.65ms
step:318/500 train_loss:4.5097 train_time:721765ms step_avg:2343.39ms
step:319/500 train_loss:4.4185 train_time:724156ms step_avg:2343.55ms
step:320/500 train_loss:4.4524 train_time:726636ms step_avg:2343.99ms
step:321/500 train_loss:4.4582 train_time:729059ms step_avg:2344.24ms
step:322/500 train_loss:4.5205 train_time:731184ms step_avg:2343.54ms
step:323/500 train_loss:4.3993 train_time:733570ms step_avg:2343.67ms
step:324/500 train_loss:4.5019 train_time:735956ms step_avg:2343.81ms
step:325/500 train_loss:4.4811 train_time:737967ms step_avg:2342.75ms
step:326/500 train_loss:4.4520 train_time:740351ms step_avg:2342.88ms
step:327/500 train_loss:4.4689 train_time:742432ms step_avg:2342.06ms
step:328/500 train_loss:4.6408 train_time:744892ms step_avg:2342.43ms
step:329/500 train_loss:4.5484 train_time:747297ms step_avg:2342.62ms
step:330/500 train_loss:4.5093 train_time:749691ms step_avg:2342.78ms
step:331/500 train_loss:4.4328 train_time:751954ms step_avg:2342.53ms
step:332/500 train_loss:5.0202 train_time:754360ms step_avg:2342.73ms
step:333/500 train_loss:4.4067 train_time:756640ms step_avg:2342.54ms
step:334/500 train_loss:4.3540 train_time:759036ms step_avg:2342.70ms
step:335/500 train_loss:4.4887 train_time:761435ms step_avg:2342.88ms
step:336/500 train_loss:4.5114 train_time:763819ms step_avg:2343.00ms
step:337/500 train_loss:4.4719 train_time:766139ms step_avg:2342.93ms
step:338/500 train_loss:4.4788 train_time:768459ms step_avg:2342.86ms
step:339/500 train_loss:4.4757 train_time:770849ms step_avg:2343.01ms
step:340/500 train_loss:4.3948 train_time:772842ms step_avg:2341.94ms
step:341/500 train_loss:4.3617 train_time:775238ms step_avg:2342.11ms
step:342/500 train_loss:4.4099 train_time:777634ms step_avg:2342.27ms
step:343/500 train_loss:4.3984 train_time:779975ms step_avg:2342.27ms
step:344/500 train_loss:4.5456 train_time:782242ms step_avg:2342.04ms
step:345/500 train_loss:4.4352 train_time:784663ms step_avg:2342.28ms
step:346/500 train_loss:4.3977 train_time:787143ms step_avg:2342.69ms
step:347/500 train_loss:4.4740 train_time:789532ms step_avg:2342.82ms
step:348/500 train_loss:4.3635 train_time:791845ms step_avg:2342.74ms
step:349/500 train_loss:4.4319 train_time:794260ms step_avg:2342.95ms
step:350/500 train_loss:4.4375 train_time:796650ms step_avg:2343.09ms
step:351/500 train_loss:4.6419 train_time:798842ms step_avg:2342.65ms
step:352/500 train_loss:4.3285 train_time:801223ms step_avg:2342.76ms
step:353/500 train_loss:4.3706 train_time:803612ms step_avg:2342.89ms
step:354/500 train_loss:4.3925 train_time:806012ms step_avg:2343.06ms
step:355/500 train_loss:4.2846 train_time:808386ms step_avg:2343.15ms
step:356/500 train_loss:4.4442 train_time:810762ms step_avg:2343.24ms
step:357/500 train_loss:4.3409 train_time:812865ms step_avg:2342.55ms
step:358/500 train_loss:4.4329 train_time:815269ms step_avg:2342.73ms
step:359/500 train_loss:4.4542 train_time:817736ms step_avg:2343.08ms
step:360/500 train_loss:4.3391 train_time:820128ms step_avg:2343.22ms
step:361/500 train_loss:4.6216 train_time:822445ms step_avg:2343.15ms
step:362/500 train_loss:4.5404 train_time:824739ms step_avg:2343.01ms
step:363/500 train_loss:4.4242 train_time:827134ms step_avg:2343.16ms
step:364/500 train_loss:4.3238 train_time:829532ms step_avg:2343.31ms
step:365/500 train_loss:4.4870 train_time:831960ms step_avg:2343.55ms
step:366/500 train_loss:4.3580 train_time:834258ms step_avg:2343.42ms
step:367/500 train_loss:4.3972 train_time:836656ms step_avg:2343.58ms
step:368/500 train_loss:4.3658 train_time:838965ms step_avg:2343.48ms
step:369/500 train_loss:4.4390 train_time:841345ms step_avg:2343.58ms
step:370/500 train_loss:4.3775 train_time:843809ms step_avg:2343.91ms
step:371/500 train_loss:4.2244 train_time:846024ms step_avg:2343.56ms
step:372/500 train_loss:4.3461 train_time:848351ms step_avg:2343.51ms
step:373/500 train_loss:4.3809 train_time:850418ms step_avg:2342.75ms
step:374/500 train_loss:4.3599 train_time:852815ms step_avg:2342.90ms
step:375/500 train_loss:4.4161 train_time:855228ms step_avg:2343.09ms
step:376/500 train_loss:4.3815 train_time:857609ms step_avg:2343.20ms
step:377/500 train_loss:4.2760 train_time:859927ms step_avg:2343.13ms
step:378/500 train_loss:3.7192 train_time:862317ms step_avg:2343.25ms
step:379/500 train_loss:4.1947 train_time:864604ms step_avg:2343.10ms
step:380/500 train_loss:4.2394 train_time:867836ms step_avg:2345.50ms
step:381/500 train_loss:4.3533 train_time:870379ms step_avg:2346.04ms
step:382/500 train_loss:4.4034 train_time:872777ms step_avg:2346.18ms
step:383/500 train_loss:4.3853 train_time:875178ms step_avg:2346.32ms
step:384/500 train_loss:4.2927 train_time:877340ms step_avg:2345.83ms
step:385/500 train_loss:4.3951 train_time:879796ms step_avg:2346.12ms
step:386/500 train_loss:4.3008 train_time:882185ms step_avg:2346.24ms
step:387/500 train_loss:4.4315 train_time:884586ms step_avg:2346.38ms
step:388/500 train_loss:4.6276 train_time:886823ms step_avg:2346.09ms
step:389/500 train_loss:4.3343 train_time:889177ms step_avg:2346.11ms
step:390/500 train_loss:4.2885 train_time:891574ms step_avg:2346.25ms
step:391/500 train_loss:4.4210 train_time:893957ms step_avg:2346.34ms
step:392/500 train_loss:4.3360 train_time:896216ms step_avg:2346.12ms
step:393/500 train_loss:4.4445 train_time:898612ms step_avg:2346.25ms
step:394/500 train_loss:4.2645 train_time:901009ms step_avg:2346.38ms
step:395/500 train_loss:4.3971 train_time:903409ms step_avg:2346.52ms
step:396/500 train_loss:4.1801 train_time:905824ms step_avg:2346.69ms
step:397/500 train_loss:4.3436 train_time:908062ms step_avg:2346.41ms
step:398/500 train_loss:4.4434 train_time:910263ms step_avg:2346.04ms
step:399/500 train_loss:4.3751 train_time:912651ms step_avg:2346.15ms
step:400/500 train_loss:4.3129 train_time:915130ms step_avg:2346.49ms
step:401/500 train_loss:4.3752 train_time:917567ms step_avg:2346.72ms
step:402/500 train_loss:4.4148 train_time:919959ms step_avg:2346.83ms
step:403/500 train_loss:4.3749 train_time:922240ms step_avg:2346.67ms
step:404/500 train_loss:4.4735 train_time:924629ms step_avg:2346.77ms
step:405/500 train_loss:4.2547 train_time:927084ms step_avg:2347.05ms
step:406/500 train_loss:4.3002 train_time:929351ms step_avg:2346.85ms
step:407/500 train_loss:4.5787 train_time:931770ms step_avg:2347.03ms
step:408/500 train_loss:4.3452 train_time:934154ms step_avg:2347.12ms
step:409/500 train_loss:4.3386 train_time:936548ms step_avg:2347.24ms
step:410/500 train_loss:4.3827 train_time:938547ms step_avg:2346.37ms
step:411/500 train_loss:4.2728 train_time:940952ms step_avg:2346.51ms
step:412/500 train_loss:4.2929 train_time:943345ms step_avg:2346.63ms
step:413/500 train_loss:4.6991 train_time:945678ms step_avg:2346.60ms
step:414/500 train_loss:4.1584 train_time:948081ms step_avg:2346.74ms
step:415/500 train_loss:4.5201 train_time:950580ms step_avg:2347.11ms
step:416/500 train_loss:4.2910 train_time:953008ms step_avg:2347.31ms
step:417/500 train_loss:4.2904 train_time:955418ms step_avg:2347.46ms
step:418/500 train_loss:4.4736 train_time:957761ms step_avg:2347.45ms
step:419/500 train_loss:4.2048 train_time:959855ms step_avg:2346.83ms
step:420/500 train_loss:4.3068 train_time:962161ms step_avg:2346.73ms
step:421/500 train_loss:4.2738 train_time:964545ms step_avg:2346.82ms
step:422/500 train_loss:4.1641 train_time:966949ms step_avg:2346.96ms
step:423/500 train_loss:4.2736 train_time:969201ms step_avg:2346.73ms
step:424/500 train_loss:4.3832 train_time:971295ms step_avg:2346.12ms
step:425/500 train_loss:4.1837 train_time:973790ms step_avg:2346.48ms
step:426/500 train_loss:4.3407 train_time:976193ms step_avg:2346.62ms
step:427/500 train_loss:4.2328 train_time:978494ms step_avg:2346.51ms
step:428/500 train_loss:4.4139 train_time:980901ms step_avg:2346.65ms
step:429/500 train_loss:4.3560 train_time:983286ms step_avg:2346.74ms
step:430/500 train_loss:4.2737 train_time:985763ms step_avg:2347.05ms
step:431/500 train_loss:4.2481 train_time:988162ms step_avg:2347.18ms
step:432/500 train_loss:4.1979 train_time:990324ms step_avg:2346.74ms
step:433/500 train_loss:4.2814 train_time:992752ms step_avg:2346.93ms
step:434/500 train_loss:4.3557 train_time:994963ms step_avg:2346.61ms
step:435/500 train_loss:4.2855 train_time:997162ms step_avg:2346.26ms
step:436/500 train_loss:4.3350 train_time:999457ms step_avg:2346.14ms
step:437/500 train_loss:4.3461 train_time:1001872ms step_avg:2346.31ms
step:438/500 train_loss:4.2240 train_time:1004088ms step_avg:2346.00ms
step:439/500 train_loss:4.2490 train_time:1006512ms step_avg:2346.18ms
step:440/500 train_loss:4.2218 train_time:1008794ms step_avg:2346.03ms
step:441/500 train_loss:4.3996 train_time:1011198ms step_avg:2346.17ms
step:442/500 train_loss:4.3008 train_time:1013587ms step_avg:2346.27ms
step:443/500 train_loss:4.2776 train_time:1015982ms step_avg:2346.38ms
step:444/500 train_loss:4.1707 train_time:1018389ms step_avg:2346.52ms
step:445/500 train_loss:4.4313 train_time:1020367ms step_avg:2345.67ms
step:446/500 train_loss:4.3479 train_time:1022772ms step_avg:2345.81ms
step:447/500 train_loss:4.3523 train_time:1025182ms step_avg:2345.95ms
step:448/500 train_loss:4.2660 train_time:1027631ms step_avg:2346.19ms
step:449/500 train_loss:4.3524 train_time:1029897ms step_avg:2346.01ms
step:450/500 train_loss:4.1895 train_time:1032287ms step_avg:2346.11ms
step:451/500 train_loss:4.2182 train_time:1034426ms step_avg:2345.64ms
step:452/500 train_loss:4.1172 train_time:1036883ms step_avg:2345.89ms
step:453/500 train_loss:4.2159 train_time:1039285ms step_avg:2346.02ms
step:454/500 train_loss:4.1993 train_time:1041647ms step_avg:2346.05ms
step:455/500 train_loss:4.1773 train_time:1043895ms step_avg:2345.83ms
step:456/500 train_loss:4.3774 train_time:1046289ms step_avg:2345.94ms
step:457/500 train_loss:4.2349 train_time:1048685ms step_avg:2346.05ms
step:458/500 train_loss:4.3225 train_time:1050977ms step_avg:2345.93ms
step:459/500 train_loss:4.3591 train_time:1053082ms step_avg:2345.39ms
step:460/500 train_loss:4.1569 train_time:1055361ms step_avg:2345.25ms
step:461/500 train_loss:4.3336 train_time:1057768ms step_avg:2345.38ms
step:462/500 train_loss:4.2314 train_time:1060168ms step_avg:2345.50ms
step:463/500 train_loss:4.2143 train_time:1062395ms step_avg:2345.24ms
step:464/500 train_loss:4.3061 train_time:1064802ms step_avg:2345.38ms
step:465/500 train_loss:4.2430 train_time:1067176ms step_avg:2345.44ms
step:466/500 train_loss:4.2389 train_time:1069582ms step_avg:2345.57ms
step:467/500 train_loss:4.3677 train_time:1071636ms step_avg:2344.94ms
step:468/500 train_loss:4.3700 train_time:1074026ms step_avg:2345.03ms
step:469/500 train_loss:4.3331 train_time:1076441ms step_avg:2345.19ms
step:470/500 train_loss:4.2444 train_time:1078828ms step_avg:2345.28ms
step:471/500 train_loss:4.3270 train_time:1081047ms step_avg:2345.00ms
step:472/500 train_loss:4.3728 train_time:1083439ms step_avg:2345.11ms
step:473/500 train_loss:4.2819 train_time:1085822ms step_avg:2345.19ms
step:474/500 train_loss:4.2551 train_time:1088223ms step_avg:2345.31ms
step:475/500 train_loss:4.1341 train_time:1090618ms step_avg:2345.42ms
step:476/500 train_loss:4.5570 train_time:1093019ms step_avg:2345.53ms
step:477/500 train_loss:4.3120 train_time:1095409ms step_avg:2345.63ms
step:478/500 train_loss:4.1201 train_time:1097798ms step_avg:2345.72ms
step:479/500 train_loss:4.3213 train_time:1100213ms step_avg:2345.87ms
step:480/500 train_loss:4.3028 train_time:1102483ms step_avg:2345.71ms
step:481/500 train_loss:4.4270 train_time:1104870ms step_avg:2345.80ms
step:482/500 train_loss:4.2524 train_time:1107227ms step_avg:2345.82ms
step:483/500 train_loss:4.0706 train_time:1109647ms step_avg:2345.98ms
step:484/500 train_loss:4.3451 train_time:1112055ms step_avg:2346.11ms
step:485/500 train_loss:4.1919 train_time:1114049ms step_avg:2345.37ms
step:486/500 train_loss:4.2230 train_time:1116222ms step_avg:2345.00ms
step:487/500 train_loss:4.1660 train_time:1118036ms step_avg:2343.89ms
step:488/500 train_loss:4.1916 train_time:1120331ms step_avg:2343.79ms
step:489/500 train_loss:4.4034 train_time:1122718ms step_avg:2343.88ms
step:490/500 train_loss:4.2544 train_time:1125102ms step_avg:2343.96ms
step:491/500 train_loss:4.1540 train_time:1127568ms step_avg:2344.22ms
step:492/500 train_loss:4.1672 train_time:1129886ms step_avg:2344.16ms
step:493/500 train_loss:4.2764 train_time:1131670ms step_avg:2343.00ms
step:494/500 train_loss:4.1233 train_time:1134071ms step_avg:2343.12ms
step:495/500 train_loss:4.2733 train_time:1136472ms step_avg:2343.24ms
step:496/500 train_loss:4.1874 train_time:1138943ms step_avg:2343.50ms
step:497/500 train_loss:4.1328 train_time:1141145ms step_avg:2343.21ms
step:498/500 train_loss:4.2840 train_time:1143549ms step_avg:2343.34ms
step:499/500 train_loss:4.3638 train_time:1146021ms step_avg:2343.60ms
step:500/500 train_loss:4.4196 train_time:1148249ms step_avg:2343.36ms
step:500/500 val_loss:4.2618 train_time:1148253ms step_avg:2343.37ms


==============================================
Final Results for run_id: 2c103f6e-168b-4cd6-8ab3-2130af9d5cf9, expansion_factor: 3.5, final_val_loss: 4.261754989624023, training_time_ms: 1148253.4296512604, peak_memory_mib: 30235
==============================================